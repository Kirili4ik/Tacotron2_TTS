{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework №4\n",
    "This homework will be dedicated to the Text-to-Speech(**TTS**), specifically the acoustic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(21)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(wav, sr, text):\n",
    "    print(text)\n",
    "    display.display(display.Audio(wav, rate=sr, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#win_len=1024, hop_len=256\n",
    "def get_mel_lens(audio_lens):\n",
    "    mel_lens = []\n",
    "    for len_now in audio_lens:\n",
    "        mel_lens.append(int((len_now - 1024)/256) + (1024//256)+1)\n",
    "    return mel_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MelSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MelSpectrogramConfig:\n",
    "    sr: int = 22050\n",
    "    win_length: int = 1024\n",
    "    hop_length: int = 256\n",
    "    n_fft: int = 1024\n",
    "    f_min: int = 0\n",
    "    f_max: int = 8000\n",
    "    n_mels: int = 80\n",
    "    power: float = 1.0\n",
    "        \n",
    "    # value of melspectrograms if we fed a silence into `MelSpectrogram`\n",
    "    pad_value: float = -11.5129251\n",
    "\n",
    "\n",
    "class MelSpectrogram(nn.Module):\n",
    "\n",
    "    def __init__(self, config: MelSpectrogramConfig):\n",
    "        super(MelSpectrogram, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=config.sr,\n",
    "            win_length=config.win_length,\n",
    "            hop_length=config.hop_length,\n",
    "            n_fft=config.n_fft,\n",
    "            f_min=config.f_min,\n",
    "            f_max=config.f_max,\n",
    "            n_mels=config.n_mels\n",
    "        )\n",
    "\n",
    "        # The is no way to set power in constructor in 0.5.0 version.\n",
    "        self.mel_spectrogram.spectrogram.power = config.power\n",
    "\n",
    "        # Default `torchaudio` mel basis uses HTK formula. In order to be compatible with WaveGlow\n",
    "        # we decided to use Slaney one instead (as well as `librosa` does by default).\n",
    "        mel_basis = librosa.filters.mel(\n",
    "            sr=config.sr,\n",
    "            n_fft=config.n_fft,\n",
    "            n_mels=config.n_mels,\n",
    "            fmin=config.f_min,\n",
    "            fmax=config.f_max\n",
    "        ).T\n",
    "        self.mel_spectrogram.mel_scale.fb.copy_(torch.tensor(mel_basis))\n",
    "    \n",
    "\n",
    "    def forward(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param audio: Expected shape is [B, T]\n",
    "        :return: Shape is [B, n_mels, T']\n",
    "        \"\"\"\n",
    "        \n",
    "        mel = self.mel_spectrogram(audio) \\\n",
    "            .clamp_(min=1e-5) \\\n",
    "            .log_()\n",
    "\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = MelSpectrogram(MelSpectrogramConfig()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the following Vocoder for final quality validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Vocoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Vocoder, self).__init__()\n",
    "        \n",
    "        model = torch.load('waveglow_256channels_universal_v5.pt', map_location='cpu')['model']\n",
    "        self.net = model.remove_weightnorm(model)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, spect: torch.Tensor):\n",
    "        spect = self.net.upsample(spect)\n",
    "        \n",
    "        # trim the conv artifacts\n",
    "        time_cutoff = self.net.upsample.kernel_size[0] - self.net.upsample.stride[0]\n",
    "        spect = spect[:, :, :-time_cutoff]\n",
    "        \n",
    "        spect = spect.unfold(2, self.net.n_group, self.net.n_group) \\\n",
    "            .permute(0, 2, 1, 3) \\\n",
    "            .contiguous() \\\n",
    "            .flatten(start_dim=2) \\\n",
    "            .transpose(-1, -2)\n",
    "        \n",
    "        # generate prior\n",
    "        audio = torch.randn(spect.size(0), self.net.n_remaining_channels, spect.size(-1)) \\\n",
    "            .to(spect.device)\n",
    "        \n",
    "        for k in reversed(range(self.net.n_flows)):\n",
    "            n_half = int(audio.size(1) / 2)\n",
    "            audio_0 = audio[:, :n_half, :]\n",
    "            audio_1 = audio[:, n_half:, :]\n",
    "\n",
    "            output = self.net.WN[k]((audio_0, spect))\n",
    "\n",
    "            s = output[:, n_half:, :]\n",
    "            b = output[:, :n_half, :]\n",
    "            audio_1 = (audio_1 - b) / torch.exp(s)\n",
    "            audio = torch.cat([audio_0, audio_1], 1)\n",
    "\n",
    "            audio = self.net.convinv[k](audio, reverse=True)\n",
    "\n",
    "            if k % self.net.n_early_every == 0 and k > 0:\n",
    "                z = torch.randn(spect.size(0), self.net.n_early_size, spect.size(2))\n",
    "                audio = torch.cat((z, audio), 1)\n",
    "\n",
    "        audio = audio.permute(0, 2, 1) \\\n",
    "            .contiguous() \\\n",
    "            .view(audio.size(0), -1)\n",
    "        \n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder = Vocoder().to(device)\n",
    "vocoder = vocoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use text from `Normalized Transcription` field in transcripts.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharsInd:\n",
    "    def __init__(self):\n",
    "        self.char2ind = {}\n",
    "        self.char2count = {}\n",
    "        self.ind2char = {}\n",
    "        self.n_chars = 1     # PAD\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for char in sentence:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2ind:\n",
    "            self.char2ind[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.ind2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset containing text and audio.\"\"\"\n",
    "\n",
    "    def __init__(self, root='LJSpeech-1.1/', csv_path='metadata.csv', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root (string): Directory with all the data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root = root\n",
    "        self.csv = pd.read_csv(root+csv_path, sep='|', header=None)\n",
    "        self.csv = self.csv.drop(columns=[1]).rename(columns={0:'filename', 2:'norm_text'})  # leave only normilized\n",
    "        self.csv = self.csv.dropna().reset_index()\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.chars_indexed = CharsInd()\n",
    "        for i in range(self.csv.shape[0]):\n",
    "            self.chars_indexed.addSentence(self.csv.loc[i, 'norm_text'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt_name = self.root + 'wavs/' + self.csv.loc[idx, 'filename'] + '.wav'\n",
    "        utt = torchaudio.load(utt_name)[0].squeeze()\n",
    "        norm_text = self.csv.loc[idx, 'norm_text']\n",
    "        \n",
    "        text_ohed = torch.Tensor(\n",
    "                            [self.chars_indexed.char2ind[char_now] for char_now in norm_text]\n",
    "                    ).type(torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            utt = self.transform(utt)\n",
    "\n",
    "        sample = {'utt': utt, 'norm_text': norm_text, 'text_ohed':text_ohed}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "    wavs, wavs_lens = [], []\n",
    "    texts = []\n",
    "    texts_ohed, texts_ohed_lens = [], []    \n",
    "        \n",
    "    for el in data:\n",
    "        wavs.append(el['utt'])\n",
    "        wavs_lens.append(len(el['utt']))\n",
    "        texts.append(el['norm_text'])\n",
    "        texts_ohed.append(el['text_ohed'])\n",
    "        texts_ohed_lens.append(len(el['text_ohed']))\n",
    "    \n",
    "    wavs_padded = torch.nn.utils.rnn.pad_sequence(wavs, batch_first=True)\n",
    "    texts_ohed_padded = torch.nn.utils.rnn.pad_sequence(texts_ohed, batch_first=True, padding_value=75)\n",
    "    \n",
    "    return wavs_padded, wavs_lens, texts_ohed_padded, texts_ohed_lens, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets + Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = TextAudioDataset(csv_path='metadata.csv', transform=None)\n",
    "my_dataset_size = len(my_dataset)\n",
    "print('all train+val samples:', my_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(my_dataset_size * 0.8 )\n",
    "val_len = my_dataset_size - train_len\n",
    "train_set, val_set = torch.utils.data.random_split(my_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, \n",
    "                          shuffle=False, collate_fn=my_collate_fn,\n",
    "                          num_workers=1, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, collate_fn=my_collate_fn, \n",
    "                        num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loaders check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''for i, batch in enumerate(train_loader):\n",
    "    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n",
    "    \n",
    "    print(utt.size(), utt_lens)\n",
    "    print(text_ohed.size(), text_ohed_lens)\n",
    "    \n",
    "    viz(utts[0], 22050, text[0])\n",
    "    if i == 2:\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''enc = Encoder(input_size=my_dataset.chars_indexed.n_chars)\n",
    "for i, batch in enumerate(train_loader):\n",
    "        \n",
    "    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n",
    "    \n",
    "    true_mel_lens = get_mel_lens(utts_lens)                   # из-за сдвига\n",
    "    true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   ### ???\n",
    "    max_true_mel_len = true_mel_lens.max()\n",
    "        \n",
    "        \n",
    "    res, _ = enc(text_ohed, text_ohed_lens)\n",
    "    print(res.size())\n",
    "    \n",
    "    viz(utt[0], 22050, text[0])\n",
    "    if i == 2:\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Tacotron2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn_act(nn.Module):\n",
    "    def __init__(self, in_channels=512, out_channels=512, kernel_s=5, act=nn.Identity(), d_rate=0.5):\n",
    "        super(conv_bn_act, self).__init__()\n",
    "        \n",
    "        # batched 2d input, but first dim is considered channels, so kernel_size is actually (in_channels, kernel_s) \n",
    "        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n",
    "                              kernel_size=kernel_s, padding=int((kernel_s - 1)/2))\n",
    "        self.b_norm = nn.BatchNorm1d(out_channels)\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(d_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.dropout(self.act(\n",
    "                    self.b_norm(self.conv(inputs))\n",
    "               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size=512, lstm_hidden_size=256):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedder = nn.Embedding(input_size, emb_size, padding_idx=75)\n",
    "        self.convs = nn.ModuleList([conv_bn_act(in_channels=emb_size, out_channels=emb_size, act=nn.ReLU())\n",
    "                                    for i in range(3)])\n",
    "        self.bi_lstm = nn.LSTM(input_size=emb_size,                  # ???\n",
    "                               hidden_size=lstm_hidden_size,\n",
    "                               batch_first=True,                     # ???\n",
    "                               bidirectional=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, true_lens):\n",
    "        # (BS, seq_len)\n",
    "        x = self.embedder(inputs).transpose(1, 2)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        # (BS, seq_len, emb_size)\n",
    "        \n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=true_lens, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        encoded, _ = self.bi_lstm(x)\n",
    "        \n",
    "        encoded_unpacked = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        \n",
    "        return encoded_unpacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LS-attention with new names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationBlock(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_n_filters,\n",
    "        attention_kernel_size,\n",
    "        attention_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        padding = int((attention_kernel_size - 1) / 2)\n",
    "        self.conv = nn.Conv1d(\n",
    "            2, attention_n_filters, kernel_size=attention_kernel_size,\n",
    "            padding=padding, bias=False\n",
    "        )\n",
    "        self.projection = nn.Linear(attention_n_filters, attention_dim, bias=False)\n",
    "    \n",
    "    def forward(self, attention_weights):\n",
    "        output = self.conv(attention_weights).transpose(1, 2)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationSensitiveAttention(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        lstm_hidden_size,\n",
    "        attention_dim,\n",
    "        attention_location_n_filters,\n",
    "        attention_location_kernel_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_layer = nn.Linear(lstm_hidden_size, attention_dim, bias=False)\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.location_layer = LocationBlock(\n",
    "            attention_location_n_filters,\n",
    "            attention_location_kernel_size,\n",
    "            attention_dim\n",
    "        )\n",
    "        self.score_mask_value = -float(\"inf\")\n",
    "        \n",
    "    def get_alignment_energies(\n",
    "        self,\n",
    "        fir_lstm_hidden,\n",
    "        processed_memory,\n",
    "        alphas_concat\n",
    "    ):\n",
    "        \"\"\"\n",
    "        fir_lstm_hidden: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
    "        processed_memory: processed encoder outputs (B, ???, attention_dim)\n",
    "        alphas_concat: cumulative and prev. att weights (B, 2, max_time)\n",
    "        \"\"\"\n",
    "        processed_query = self.query_layer(fir_lstm_hidden.unsqueeze(1))\n",
    "        processed_alphas_concat = self.location_layer(alphas_concat)\n",
    "        \n",
    "        energies = self.v(torch.tanh(\n",
    "            processed_query + processed_alphas_concat + processed_memory\n",
    "        ))\n",
    "\n",
    "        energies = energies.squeeze(2)\n",
    "        return energies\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        enc_outputs,\n",
    "        processed_memory,\n",
    "        fir_lstm_hidden,\n",
    "        alphas_concat,\n",
    "        mask=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        enc_outputs: encoder outputs\n",
    "        processed_memory: processed encoder outputs\n",
    "        fir_lstm_hidden: attention(first) rnn last output\n",
    "        alphas_concat: previous and cummulative attention weights\n",
    "        mask: binary mask for padded data\n",
    "        \"\"\"\n",
    "        \n",
    "        alignment = self.get_alignment_energies(\n",
    "            fir_lstm_hidden, processed_memory, alphas_concat\n",
    "        )\n",
    "\n",
    "        alignment = alignment.masked_fill(mask, self.score_mask_value) if mask is not None else alignment\n",
    "\n",
    "        attention_weights = F.softmax(alignment, dim=1)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), enc_outputs)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prenet_layer(nn.Module):\n",
    "    \"\"\"1 layer of (Linear, Relu, Dropout). 2 of this for real prenet layer in Tacotron2\"\"\"\n",
    "    \n",
    "    def __init__(self, in_size=256, out_size=256, d_rate=0.5):\n",
    "        super(prenet_layer, self).__init__()\n",
    "        \n",
    "        self.proj = nn.Linear(in_features=in_size, out_features=out_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(d_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.dropout(self.relu(self.proj(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, prenet_size=256, att_size=512, lstm_hid_size=1024, after_lstm_d_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.prenet = nn.Sequential(prenet_layer(in_size=80, out_size=prenet_size), \n",
    "                                    prenet_layer(in_size=prenet_size, out_size=prenet_size))\n",
    "        self.fir_LSTMCell = nn.LSTMCell(input_size=prenet_size+att_size, hidden_size=lstm_hid_size)\n",
    "        self.fir_dropout  = nn.Dropout(after_lstm_d_rate)\n",
    "        \n",
    "        ### 128 is attention hidden kinda\n",
    "        self.proj_memory  = nn.Linear(prenet_size*2, 128)\n",
    "        self.LS_attention = LocationSensitiveAttention(lstm_hid_size, 128, 32, 31)\n",
    "        \n",
    "        self.sec_LSTMCell = nn.LSTMCell(input_size=lstm_hid_size+att_size, hidden_size=lstm_hid_size)\n",
    "        self.sec_dropout  = nn.Dropout(after_lstm_d_rate)\n",
    "        self.proj_mel     = nn.Linear(lstm_hid_size+att_size, 80)\n",
    "        self.proj_exit    = nn.Linear(lstm_hid_size+att_size, 1)\n",
    "        \n",
    "        postnet_list = [conv_bn_act(in_channels=80, out_channels=512, act=nn.Tanh())]\n",
    "        for i in range(5-2):\n",
    "            postnet_list.append(\n",
    "                conv_bn_act(in_channels=512, out_channels=512, act=nn.Tanh())\n",
    "            )\n",
    "        postnet_list.append(conv_bn_act(in_channels=512, out_channels=80, act=nn.Identity()))\n",
    "        self.postnet = nn.Sequential(*postnet_list)   # * just unpacks list \n",
    "        \n",
    "       \n",
    "    \n",
    "    def forward(self, enc_outputs, true_mels, true_lens, max_len, is_inference=False):\n",
    "        BS = enc_outputs.size(0)\n",
    "        seq_len = enc_outputs.size(1)\n",
    "        processed_memory = self.proj_memory(enc_outputs)\n",
    "        # create mask from true_lens for chars: True <=> PAD, not use\n",
    "        true_lens = torch.Tensor(true_lens).type(torch.long)\n",
    "        att_mask = torch.arange(max(true_lens)).repeat(BS, 1) >= true_lens.unsqueeze(1)\n",
    "        att_mask = att_mask.to(enc_outputs.device)\n",
    "        #print('att mask size', att_mask.size())\n",
    "        #print('enc outputs in att size', enc_outputs.size())\n",
    "        \n",
    "        device = enc_outputs.device\n",
    "        \n",
    "        prenet_output = torch.zeros((BS, 256), device=device)         # всегда?\n",
    "        att_context    = torch.zeros((BS, 512), device=device)\n",
    "        fir_lstm_hidden = torch.zeros((BS, 1024), device=device)\n",
    "        fir_lstm_context = torch.zeros((BS, 1024), device=device)\n",
    "        alphas         = torch.zeros((BS, seq_len), device=device)\n",
    "        alphas_sum     = torch.zeros((BS, seq_len), device=device)\n",
    "        att_mat        = torch.zeros((BS, seq_len, 1), device=device)\n",
    "        sec_lstm_hidden = torch.zeros((BS, 1024), device=device)\n",
    "        sec_lstm_context = torch.zeros((BS, 1024), device=device)\n",
    "        \n",
    "        output_exits, output_mels = [], []\n",
    "        for frame_num in range(max_len):\n",
    "            \n",
    "            ### concat + 1_lstm + 1_dropout\n",
    "            fir_lstm_inputs = torch.cat((prenet_output, att_context), dim=1)  # в длинну 256+512\n",
    "            fir_lstm_hidden, fir_lstm_context = self.fir_LSTMCell(fir_lstm_inputs, (fir_lstm_hidden, fir_lstm_context))\n",
    "            fir_lstm_hidden = self.fir_dropout(fir_lstm_hidden)\n",
    "            #print('1', fir_lstm_hidden.size(), fir_lstm_context.size())\n",
    "\n",
    "\n",
    "            ### concat + LS_att\n",
    "            alphas_concat = torch.stack((alphas, alphas_sum), dim=-1).transpose(1, 2)  # по новому дименшну\n",
    "            \n",
    "            att_context, alphas = self.LS_attention(enc_outputs, processed_memory, fir_lstm_hidden, alphas_concat,\n",
    "                                                    mask=att_mask)\n",
    "            alphas_sum = alphas_sum + alphas\n",
    "            att_mat = torch.cat((att_mat, alphas.unsqueeze(2)), dim=2)              #### CHECK\n",
    "            #print('2', att_context.size(), alphas_concat.size())\n",
    "\n",
    "\n",
    "            ### concat + 2_lstm\n",
    "            sec_lstm_inputs = torch.cat((fir_lstm_hidden, att_context), dim=1)  # в длину 1024+512\n",
    "            sec_lstm_hidden, sec_lstm_context = self.sec_LSTMCell(sec_lstm_inputs, (sec_lstm_hidden, sec_lstm_context))\n",
    "            sec_lstm_hidden = self.sec_dropout(sec_lstm_hidden)\n",
    "            #print('3', sec_lstm_hidden.size(), sec_lstm_context.size())\n",
    "\n",
    "            ### concat + FC x 2\n",
    "            FC_inputs = torch.cat((sec_lstm_hidden, att_context), dim=1)   # в длину?\n",
    "            output_mel = self.proj_mel(FC_inputs)\n",
    "            output_mels.append(output_mel)\n",
    "            output_exit = F.sigmoid(self.proj_exit(FC_inputs))\n",
    "            output_exits.append(output_exit)\n",
    "            #print('4', output_mel.size(), output_exit.size())\n",
    "            \n",
    "            ### Prenet\n",
    "            if is_inference:\n",
    "                prenet_output = self.prenet(output_mel)\n",
    "                # output exit is (1,1) sized cause BS=1\n",
    "                if output_exit.item() > 0.5:\n",
    "                    break\n",
    "            else:\n",
    "                prenet_output = self.prenet(true_mels[:, :, frame_num])\n",
    "            #print('5', prenet_output.size())\n",
    "            \n",
    "        \n",
    "        ### Postnet\n",
    "        output_mels = torch.stack(output_mels, dim=-1) ### (BS, 80, num_frames)\n",
    "        postnet_res = self.postnet(output_mels)   #(вроде сделано) эта свертка должна быть ПО ФРЕЙМАМ, каждый прогон цикла дает фрейм (1,80)  \n",
    "        post_output_mel = postnet_res + output_mels\n",
    "        \n",
    "        # exits\n",
    "        output_exits = torch.cat(output_exits, dim=1)\n",
    "        \n",
    "        return output_mels, output_exits, post_output_mel, att_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FullModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_size=input_size)\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    \n",
    "    def forward(self, text_oheds, true_mels, text_lens, max_len):\n",
    "        \n",
    "        enc_outputs, _ = self.encoder(text_oheds, text_lens)\n",
    "\n",
    "        output_mels, output_exits, post_output_mel, att_mat = self.decoder(enc_outputs, true_mels, text_lens, max_len)\n",
    "        \n",
    "        return output_mels, output_exits, post_output_mel, att_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix for guided-attention\n",
    "def W_loss_mat(dims, text_len, mel_len, g=0.2):\n",
    "    padded_T, padded_N = dims\n",
    "    N, T = mel_len, text_len\n",
    "    mat = torch.zeros((padded_N, padded_T))\n",
    "    for n in range(N):\n",
    "        t = np.arange(T)\n",
    "        mat[N-n-1, padded_T-T:] = torch.Tensor(1 - np.exp(- (n/N - t/T)**2 / (2*g**2)))\n",
    "    return mat.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacotron = Tacotron2(input_size=my_dataset.chars_indexed.n_chars).to(device)\n",
    "count_parameters(tacotron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(tacotron.parameters(), weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "!wandb login 6aa2251ef1ea5e572e6a7608c0152db29bd9a294\n",
    "\n",
    "wandb.init(project='dla-ht4')\n",
    "wandb.watch(tacotron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tacotron.train()\n",
    "\n",
    "for ep_num in range(0, 25):\n",
    "    \n",
    "    print('epoch num now will be', ep_num)\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n",
    "        utt, text_ohed = utt.to(device), text_ohed.to(device)\n",
    "\n",
    "\n",
    "        true_mel_lens = get_mel_lens(utt_lens)\n",
    "        true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   # из-за сдвига\n",
    "        max_true_mel_len = true_mel_lens.max()\n",
    "        \n",
    "\n",
    "        # masking\n",
    "        exit_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= true_mel_lens.unsqueeze(1)\n",
    "        exit_mask = exit_mask.to(device)\n",
    "        nullify_mask = ~exit_mask\n",
    "        nullify_mask = nullify_mask.repeat(80, 1, 1).transpose(0, 1)\n",
    "\n",
    "        true_mels = featurizer(utt)    \n",
    "        true_mels = true_mels[:, :, :-1]  # сдвиг для inputs модели\n",
    "        \n",
    "        output_mels, output_exits, post_output_mel, att_mat = tacotron(text_ohed, true_mels, text_ohed_lens, max_true_mel_len)\n",
    "\n",
    "        ### Losses\n",
    "        output_mels = output_mels * nullify_mask\n",
    "        true_mels = true_mels * nullify_mask\n",
    "        L_pre  = F.mse_loss(output_mels, true_mels)\n",
    "\n",
    "\n",
    "        # masking for CE \n",
    "        # CE mask 1 less all than lens\n",
    "        ce_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= (true_mel_lens).unsqueeze(1)\n",
    "        ce_mask = ce_mask.to(device)\n",
    "        zeros = torch.zeros((utt.size(0), max_true_mel_len)).to(device)\n",
    "        exit_CE_ans = zeros.masked_fill(ce_mask, 1)\n",
    "\n",
    "        L_exit = F.binary_cross_entropy(output_exits, exit_CE_ans)   ####### true is exit or not\n",
    "\n",
    "        post_output_mel = post_output_mel * nullify_mask\n",
    "        L_post = F.mse_loss(post_output_mel, true_mels)\n",
    "        \n",
    "        # GUIDED LOSS\n",
    "        W = torch.zeros_like(att_mat)\n",
    "        for b_el_num, mat in enumerate(att_mat):\n",
    "            # lens == number of True\n",
    "            first = (torch.arange((mat.size(0))).repeat(mat.size(1), 1) >= text_ohed_lens[b_el_num]).transpose(0, 1)\n",
    "            first = ~first  # True = leave element not, False = delete\n",
    "    \n",
    "            second = (torch.arange((mat.size(1))).repeat(mat.size(0), 1) >= true_mel_lens[b_el_num])\n",
    "            second = ~second\n",
    "            act_mask_now = (first*second).to(device)\n",
    "            att_mat[b_el_num] = att_mat[b_el_num] * act_mask_now\n",
    "                        \n",
    "            # return 2d of needed size but good\n",
    "            W_b_el = W_loss_mat(act_mask_now.size(), text_ohed_lens[b_el_num], true_mel_lens[b_el_num].item())  \n",
    "            W[b_el_num] = W_b_el\n",
    "        \n",
    "        W = W.to(device)\n",
    "        \n",
    "        L_guided = (att_mat*W).sum(dim=1).sum(dim=1).mean()\n",
    "\n",
    "        loss = L_pre + L_exit + L_post + L_guided        \n",
    "\n",
    "        wandb.log({'train_loss':loss,\n",
    "                   'pre':L_pre.item(), 'exit':L_exit.item(), 'post':L_post.item(), 'guided':L_guided.item()})\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(tacotron.parameters(), 10)\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "    torch.save({\n",
    "        'model_state_dict': tacotron.state_dict(),\n",
    "    }, '../../working/taco_new'+str(ep_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
