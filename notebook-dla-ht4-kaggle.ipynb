{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Homework №4\nThis homework will be dedicated to the Text-to-Speech(**TTS**), specifically the acoustic models."},{"metadata":{"trusted":true},"cell_type":"code","source":"cd ..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd dlaht4dataset/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall -y torch\n!pip uninstall -y torchaudio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torch==1.7.0 torchaudio\n!pip install wandb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython import display\nfrom dataclasses import dataclass\n\nimport random\nimport numpy as np\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport torchaudio\n\nimport librosa\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed):\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\nset_seed(21)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    return sum([np.prod(p.size()) for p in model_parameters])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def viz(wav, sr, text):\n    print(text)\n    display.display(display.Audio(wav, rate=sr, normalize=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#win_len=1024, hop_len=256\ndef get_mel_lens(audio_lens):\n    mel_lens = []\n    for len_now in audio_lens:\n        mel_lens.append(int((len_now - 1024)/256) + (1024//256)+1)\n    return mel_lens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"    In this homework we will use only LJSpeech https://keithito.com/LJ-Speech-Dataset/.\n    Use text from `Normalized Transcription` field in transcripts.csv.\n    \n    Use the following `featurizer` (his configuration is +- standard for this task):"},{"metadata":{"trusted":true},"cell_type":"code","source":"@dataclass\nclass MelSpectrogramConfig:\n    sr: int = 22050\n    win_length: int = 1024\n    hop_length: int = 256\n    n_fft: int = 1024\n    f_min: int = 0\n    f_max: int = 8000\n    n_mels: int = 80\n    power: float = 1.0\n        \n    # value of melspectrograms if we fed a silence into `MelSpectrogram`\n    pad_value: float = -11.5129251\n\n\nclass MelSpectrogram(nn.Module):\n\n    def __init__(self, config: MelSpectrogramConfig):\n        super(MelSpectrogram, self).__init__()\n        \n        self.config = config\n\n        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n            sample_rate=config.sr,\n            win_length=config.win_length,\n            hop_length=config.hop_length,\n            n_fft=config.n_fft,\n            f_min=config.f_min,\n            f_max=config.f_max,\n            n_mels=config.n_mels\n        )\n\n        # The is no way to set power in constructor in 0.5.0 version.\n        self.mel_spectrogram.spectrogram.power = config.power\n\n        # Default `torchaudio` mel basis uses HTK formula. In order to be compatible with WaveGlow\n        # we decided to use Slaney one instead (as well as `librosa` does by default).\n        mel_basis = librosa.filters.mel(\n            sr=config.sr,\n            n_fft=config.n_fft,\n            n_mels=config.n_mels,\n            fmin=config.f_min,\n            fmax=config.f_max\n        ).T\n        self.mel_spectrogram.mel_scale.fb.copy_(torch.tensor(mel_basis))\n    \n\n    def forward(self, audio: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param audio: Expected shape is [B, T]\n        :return: Shape is [B, n_mels, T']\n        \"\"\"\n        \n        mel = self.mel_spectrogram(audio) \\\n            .clamp_(min=1e-5) \\\n            .log_()\n\n        return mel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featurizer = MelSpectrogram(MelSpectrogramConfig()).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''wav, sr = torchaudio.load('../week01/audio.wav')\nwav = wav.to(device)\nmels = featurizer(wav).to(device)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''_, axes = plt.subplots(2, 1, figsize=(15, 7))\naxes[0].plot(wav.squeeze())\naxes[1].imshow(mels.squeeze())\n\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will use pretrained Vocoder from Nvidia"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!git clone https://github.com/NVIDIA/waveglow.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Download pretrained model from https://github.com/NVIDIA/waveglow"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install googledrivedownloader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from google_drive_downloader import GoogleDriveDownloader as gdd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gdd.download_file_from_google_drive(\n#    file_id='1rpK8CzAAirq9sWZhe9nlfvxMF1dRgFbF',\n#    dest_path='./waveglow_256channels_universal_v5.pt'\n#)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gdd.download_file_from_google_drive(\n#    file_id='1A-APp73B0HkFGFcvUUCfswSBAJZSbf6F',\n#    dest_path='./new_file.tar.bz1'\n#)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!tar -xf new_file.tar.bz1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use the following Vocoder for final quality validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('waveglow/')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nclass Vocoder(nn.Module):\n    \n    def __init__(self):\n        super(Vocoder, self).__init__()\n        \n        model = torch.load('waveglow_256channels_universal_v5.pt', map_location='cpu')['model']\n        self.net = model.remove_weightnorm(model)\n    \n    @torch.no_grad()\n    def inference(self, spect: torch.Tensor):\n        spect = self.net.upsample(spect)\n        \n        # trim the conv artifacts\n        time_cutoff = self.net.upsample.kernel_size[0] - self.net.upsample.stride[0]\n        spect = spect[:, :, :-time_cutoff]\n        \n        spect = spect.unfold(2, self.net.n_group, self.net.n_group) \\\n            .permute(0, 2, 1, 3) \\\n            .contiguous() \\\n            .flatten(start_dim=2) \\\n            .transpose(-1, -2)\n        \n        # generate prior\n        audio = torch.randn(spect.size(0), self.net.n_remaining_channels, spect.size(-1)) \\\n            .to(spect.device)\n        \n        for k in reversed(range(self.net.n_flows)):\n            n_half = int(audio.size(1) / 2)\n            audio_0 = audio[:, :n_half, :]\n            audio_1 = audio[:, n_half:, :]\n\n            output = self.net.WN[k]((audio_0, spect))\n\n            s = output[:, n_half:, :]\n            b = output[:, :n_half, :]\n            audio_1 = (audio_1 - b) / torch.exp(s)\n            audio = torch.cat([audio_0, audio_1], 1)\n\n            audio = self.net.convinv[k](audio, reverse=True)\n\n            if k % self.net.n_early_every == 0 and k > 0:\n                z = torch.randn(spect.size(0), self.net.n_early_size, spect.size(2))\n                audio = torch.cat((z, audio), 1)\n\n        audio = audio.permute(0, 2, 1) \\\n            .contiguous() \\\n            .view(audio.size(0), -1)\n        \n        return audio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocoder = Vocoder().to(device)\nvocoder = vocoder.eval()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test pretrained vocoder"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''wav, sr = torchaudio.load('../week01/audio.wav')\nwav = wav.to(device)\nmels = featurizer(wav).to(device)\n\nreconstructed_wav = vocoder.inference(mels)'''","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"'''plt.plot(reconstructed_wav.squeeze(), label='reconstructed', alpha=.5)\nplt.plot(wav.squeeze(), label='GT', alpha=.5)\nplt.grid()\nplt.legend()\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''display.Audio(reconstructed_wav.squeeze(), rate=22050)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''display.Audio(wav.squeeze(), rate=22050)'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"    1) You need to implement Tacotron 2: https://www.dropbox.com/s/il9ai3ihh3nbcoo/Tacotron2.pdf?dl=0\n    \n    2) For fast convergence, implement Guided Attention: https://www.dropbox.com/s/tiibpqcdb1eieg4/GaidedAttention.pdf?dl=0\n       It will increase convergence by several times!\n    \n    3) (Bonus) Implement Monotonic Attention: https://www.dropbox.com/s/6deupekgd4ep0kg/MonotonicAttention.pdf?dl=0\n    \n    4) (Bonus) Implement Tacotron with GST.\n        For testing GST find 5-10 audio references with questioning tone and generate audio with this references.\n        \n    5) (PAY ATTENTION) Tacotron has RNNs, so don't forget about length masks."},{"metadata":{},"cell_type":"markdown","source":"# Code"},{"metadata":{},"cell_type":"markdown","source":"    1) In this homework you are allowed to use pytorch-lighting.\n\n    2) Try to write code more structurally and cleanly!\n\n    3) Good logging of experiments save your nerves and time, so we ask you to use W&B.\n       Log loss, generated and real melspectrograms (in pair, i.e. real melspec and generated from correspond text). \n       Do not remove the logs until we have checked your work and given you a grade!\n\n    4) (Bonus) We also ask you to organize your code in github repo with Docker and setup.py. You can use my template https://github.com/markovka17/dl-start-pack.\n\n    5) Your work must be reproducable, so fix seed, save the weights of model, and etc.\n\n    6) In the end of your work write inference utils. Anyone should be able to take your weight, load it into the model and run it on some melspec."},{"metadata":{},"cell_type":"markdown","source":"# Report"},{"metadata":{},"cell_type":"markdown","source":"    Finally, you need to write a report in W&B https://www.wandb.com/reports. Add examples of generated mel and audio, compare with GT.\n    Don't forget to add link to your report."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use text from `Normalized Transcription` field in transcripts.csv."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CharsInd:\n    def __init__(self):\n        self.char2ind = {}\n        self.char2count = {}\n        self.ind2char = {}\n        self.n_chars = 1     # PAD\n    \n    def addSentence(self, sentence):\n        for char in sentence:\n            self.addChar(char)\n\n    def addChar(self, char):\n        if char not in self.char2ind:\n            self.char2ind[char] = self.n_chars\n            self.char2count[char] = 1\n            self.ind2char[self.n_chars] = char\n            self.n_chars += 1\n        else:\n            self.char2count[char] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextAudioDataset(torch.utils.data.Dataset):\n    \"\"\"Custom dataset containing text and audio.\"\"\"\n\n    def __init__(self, root='LJSpeech-1.1/', csv_path='metadata.csv', transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root (string): Directory with all the data.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        \n        self.root = root\n        self.csv = pd.read_csv(root+csv_path, sep='|', header=None)\n        self.csv = self.csv.drop(columns=[1]).rename(columns={0:'filename', 2:'norm_text'})  # leave only normilized\n        self.csv = self.csv.dropna().reset_index()\n        self.transform = transform\n        \n        self.chars_indexed = CharsInd()\n        for i in range(self.csv.shape[0]):\n            self.chars_indexed.addSentence(self.csv.loc[i, 'norm_text'])\n\n    def __len__(self):\n        return self.csv.shape[0]\n    \n\n    def __getitem__(self, idx):\n        utt_name = self.root + 'wavs/' + self.csv.loc[idx, 'filename'] + '.wav'\n        utt = torchaudio.load(utt_name)[0].squeeze()\n        norm_text = self.csv.loc[idx, 'norm_text']\n        \n        text_ohed = torch.Tensor(\n                            [self.chars_indexed.char2ind[char_now] for char_now in norm_text]\n                    ).type(torch.long)\n        \n        if self.transform:\n            utt = self.transform(utt)\n\n        sample = {'utt': utt, 'norm_text': norm_text, 'text_ohed':text_ohed}\n        return sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_collate_fn(data):\n    wavs, wavs_lens = [], []\n    texts = []\n    texts_ohed, texts_ohed_lens = [], []    \n        \n    for el in data:\n        wavs.append(el['utt'])\n        wavs_lens.append(len(el['utt']))\n        texts.append(el['norm_text'])\n        texts_ohed.append(el['text_ohed'])\n        texts_ohed_lens.append(len(el['text_ohed']))\n    \n    wavs_padded = torch.nn.utils.rnn.pad_sequence(wavs, batch_first=True)\n    texts_ohed_padded = torch.nn.utils.rnn.pad_sequence(texts_ohed, batch_first=True, padding_value=75)\n    \n    return wavs_padded, wavs_lens, texts_ohed_padded, texts_ohed_lens, texts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### actual building"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_dataset = TextAudioDataset(csv_path='metadata.csv', transform=None)\nmy_dataset_size = len(my_dataset)\nprint('all train+val samples:', my_dataset_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len = int(my_dataset_size * 0.8 )\nval_len = my_dataset_size - train_len\ntrain_set, val_set = torch.utils.data.random_split(my_dataset, [train_len, val_len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, \n                          shuffle=False, collate_fn=my_collate_fn,\n                          num_workers=1, pin_memory=True)\n\nval_loader = DataLoader(val_set, batch_size=BATCH_SIZE, \n                        shuffle=False, collate_fn=my_collate_fn, \n                        num_workers=1, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### loaders check"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"'''for i, batch in enumerate(train_loader):\n    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n    \n    print(utt.size(), utt_lens)\n    print(text_ohed.size(), text_ohed_lens)\n    \n    viz(utts[0], 22050, text[0])\n    if i == 2:\n        break'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Encoder check"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"'''enc = Encoder(input_size=my_dataset.chars_indexed.n_chars)\nfor i, batch in enumerate(train_loader):\n        \n    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n    \n    true_mel_lens = get_mel_lens(utts_lens)                   # из-за сдвига\n    true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   ### ???\n    max_true_mel_len = true_mel_lens.max()\n        \n        \n    res, _ = enc(text_ohed, text_ohed_lens)\n    print(res.size())\n    \n    viz(utt[0], 22050, text[0])\n    if i == 2:\n        break'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Architecture Tacotron2\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class conv_bn_act(nn.Module):\n    def __init__(self, in_channels=512, out_channels=512, kernel_s=5, act=nn.Identity(), d_rate=0.5):\n        super(conv_bn_act, self).__init__()\n        \n        # batched 2d input, but first dim is considered channels, so kernel_size is actually (in_channels, kernel_s) \n        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n                              kernel_size=kernel_s, padding=int((kernel_s - 1)/2))\n        self.b_norm = nn.BatchNorm1d(out_channels)\n        self.act = act\n        self.dropout = nn.Dropout(d_rate)\n        \n        \n    def forward(self, inputs):\n        return self.dropout(self.act(\n                    self.b_norm(self.conv(inputs))\n               ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, emb_size=512, lstm_hidden_size=256):\n        super(Encoder, self).__init__()\n        \n        self.embedder = nn.Embedding(input_size, emb_size, padding_idx=75)\n        self.convs = nn.ModuleList([conv_bn_act(in_channels=emb_size, out_channels=emb_size, act=nn.ReLU())\n                                    for i in range(3)])\n        self.bi_lstm = nn.LSTM(input_size=emb_size,                  # ???\n                               hidden_size=lstm_hidden_size,\n                               batch_first=True,                     # ???\n                               bidirectional=True)\n        \n    \n    def forward(self, inputs, true_lens):\n        # (BS, seq_len)\n        x = self.embedder(inputs).transpose(1, 2)\n        for conv in self.convs:\n            x = conv(x)\n        x = x.transpose(1, 2)\n        # (BS, seq_len, emb_size)\n        \n        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=true_lens, batch_first=True, enforce_sorted=False)\n        \n        encoded, _ = self.bi_lstm(x)\n        \n        encoded_unpacked = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n        \n        return encoded_unpacked","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LS-attention COPIED"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LocationBlock(nn.Module):\n    \n    def __init__(\n        self,\n        attention_n_filters,\n        attention_kernel_size,\n        attention_dim\n    ):\n        super().__init__()\n        \n        padding = int((attention_kernel_size - 1) / 2)\n        self.conv = nn.Conv1d(\n            2, attention_n_filters, kernel_size=attention_kernel_size,\n            padding=padding, bias=False\n        )\n        self.projection = nn.Linear(attention_n_filters, attention_dim, bias=False)\n    \n    def forward(self, attention_weights):\n        output = self.conv(attention_weights).transpose(1, 2)\n        output = self.projection(output)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LocationSensitiveAttention(nn.Module):\n    \n    def __init__(\n        self,\n        lstm_hidden_size,\n        attention_dim,\n        attention_location_n_filters,\n        attention_location_kernel_size\n    ):\n        super().__init__()\n        \n        self.query_layer = nn.Linear(lstm_hidden_size, attention_dim, bias=False)\n        self.v = nn.Linear(attention_dim, 1, bias=False)\n        self.location_layer = LocationBlock(\n            attention_location_n_filters,\n            attention_location_kernel_size,\n            attention_dim\n        )\n        self.score_mask_value = -float(\"inf\")\n        \n    def get_alignment_energies(\n        self,\n        fir_lstm_hidden,\n        processed_memory,\n        alphas_concat\n    ):\n        \"\"\"\n        fir_lstm_hidden: decoder output (batch, n_mel_channels * n_frames_per_step)\n        processed_memory: processed encoder outputs (B, ???, attention_dim)\n        alphas_concat: cumulative and prev. att weights (B, 2, max_time)\n        \"\"\"\n        processed_query = self.query_layer(fir_lstm_hidden.unsqueeze(1))\n        processed_alphas_concat = self.location_layer(alphas_concat)\n        \n        #print('LS-attention', processed_query.size(), processed_alphas_concat.size(), processed_memory.size())\n\n        energies = self.v(torch.tanh(\n            processed_query + processed_alphas_concat + processed_memory\n        ))\n\n        energies = energies.squeeze(2)\n        return energies\n    \n    def forward(\n        self,\n        enc_outputs,\n        processed_memory,\n        fir_lstm_hidden,\n        alphas_concat,\n        mask=None\n    ):\n        \"\"\"\n        enc_outputs: encoder outputs\n        processed_memory: processed encoder outputs\n        fir_lstm_hidden: attention(first) rnn last output\n        alphas_concat: previous and cummulative attention weights\n        mask: binary mask for padded data\n        \"\"\"\n        \n        alignment = self.get_alignment_energies(\n            fir_lstm_hidden, processed_memory, alphas_concat\n        )\n\n        alignment = alignment.masked_fill(mask, self.score_mask_value) if mask is not None else alignment\n\n        attention_weights = F.softmax(alignment, dim=1)\n        attention_context = torch.bmm(attention_weights.unsqueeze(1), enc_outputs)\n        attention_context = attention_context.squeeze(1)\n\n        return attention_context, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class prenet_layer(nn.Module):\n    \"\"\"1 layer of (Linear, Relu, Dropout). 2 of this for real prenet layer in Tacotron2\"\"\"\n    \n    def __init__(self, in_size=256, out_size=256, d_rate=0.5):\n        super(prenet_layer, self).__init__()\n        \n        self.proj = nn.Linear(in_features=in_size, out_features=out_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(d_rate)\n        \n    def forward(self, inputs):\n        return self.dropout(self.relu(self.proj(inputs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, prenet_size=256, att_size=512, lstm_hid_size=1024, after_lstm_d_rate=0.1):\n        super(Decoder, self).__init__()\n        \n        self.prenet = nn.Sequential(prenet_layer(in_size=80, out_size=prenet_size), \n                                    prenet_layer(in_size=prenet_size, out_size=prenet_size))\n        self.fir_LSTMCell = nn.LSTMCell(input_size=prenet_size+att_size, hidden_size=lstm_hid_size)\n        self.fir_dropout  = nn.Dropout(after_lstm_d_rate)\n        \n        ### 128 is attention hidden kinda\n        self.proj_memory  = nn.Linear(prenet_size*2, 128)\n        self.LS_attention = LocationSensitiveAttention(lstm_hid_size, 128, 32, 31)\n        \n        self.sec_LSTMCell = nn.LSTMCell(input_size=lstm_hid_size+att_size, hidden_size=lstm_hid_size)\n        self.sec_dropout  = nn.Dropout(after_lstm_d_rate)\n        self.proj_mel     = nn.Linear(lstm_hid_size+att_size, 80)\n        self.proj_exit    = nn.Linear(lstm_hid_size+att_size, 1)\n        \n        postnet_list = [conv_bn_act(in_channels=80, out_channels=512, act=nn.Tanh())]\n        for i in range(5-2):\n            postnet_list.append(\n                conv_bn_act(in_channels=512, out_channels=512, act=nn.Tanh())\n            )\n        postnet_list.append(conv_bn_act(in_channels=512, out_channels=80, act=nn.Identity()))\n        self.postnet = nn.Sequential(*postnet_list)   # * just unpacks list \n        \n       \n    \n    def forward(self, enc_outputs, true_mels, true_lens, max_len, is_inference=False):\n        BS = enc_outputs.size(0)\n        seq_len = enc_outputs.size(1)\n        processed_memory = self.proj_memory(enc_outputs)\n        # create mask from true_lens for chars: True <=> PAD, not use\n        true_lens = torch.Tensor(true_lens).type(torch.long)\n        att_mask = torch.arange(max(true_lens)).repeat(BS, 1) >= true_lens.unsqueeze(1)\n        att_mask = att_mask.to(enc_outputs.device)\n        #print('att mask size', att_mask.size())\n        #print('enc outputs in att size', enc_outputs.size())\n        \n        device = enc_outputs.device\n        \n        prenet_output = torch.zeros((BS, 256), device=device)         # всегда?\n        att_context    = torch.zeros((BS, 512), device=device)\n        fir_lstm_hidden = torch.zeros((BS, 1024), device=device)\n        fir_lstm_context = torch.zeros((BS, 1024), device=device)\n        alphas         = torch.zeros((BS, seq_len), device=device)\n        alphas_sum     = torch.zeros((BS, seq_len), device=device)\n        att_mat        = torch.zeros((BS, seq_len, 1), device=device)\n        sec_lstm_hidden = torch.zeros((BS, 1024), device=device)\n        sec_lstm_context = torch.zeros((BS, 1024), device=device)\n        \n        output_exits, output_mels = [], []\n        for frame_num in range(max_len):\n            \n            ### concat + 1_lstm + 1_dropout\n            fir_lstm_inputs = torch.cat((prenet_output, att_context), dim=1)  # в длинну 256+512\n            fir_lstm_hidden, fir_lstm_context = self.fir_LSTMCell(fir_lstm_inputs, (fir_lstm_hidden, fir_lstm_context))\n            fir_lstm_hidden = self.fir_dropout(fir_lstm_hidden)\n            #print('1', fir_lstm_hidden.size(), fir_lstm_context.size())\n\n\n            ### concat + LS_att\n            alphas_concat = torch.stack((alphas, alphas_sum), dim=-1).transpose(1, 2)  # по новому дименшну\n            \n            att_context, alphas = self.LS_attention(enc_outputs, processed_memory, fir_lstm_hidden, alphas_concat,\n                                                    mask=att_mask)\n            alphas_sum = alphas_sum + alphas\n            att_mat = torch.cat((att_mat, alphas.unsqueeze(2)), dim=2)              #### CHECK\n            #print('2', att_context.size(), alphas_concat.size())\n\n\n            ### concat + 2_lstm\n            sec_lstm_inputs = torch.cat((fir_lstm_hidden, att_context), dim=1)  # в длину 1024+512\n            sec_lstm_hidden, sec_lstm_context = self.sec_LSTMCell(sec_lstm_inputs, (sec_lstm_hidden, sec_lstm_context))\n            sec_lstm_hidden = self.sec_dropout(sec_lstm_hidden)\n            #print('3', sec_lstm_hidden.size(), sec_lstm_context.size())\n\n            ### concat + FC x 2\n            FC_inputs = torch.cat((sec_lstm_hidden, att_context), dim=1)   # в длину?\n            output_mel = self.proj_mel(FC_inputs)\n            output_mels.append(output_mel)\n            output_exit = F.sigmoid(self.proj_exit(FC_inputs))\n            output_exits.append(output_exit)\n            #print('4', output_mel.size(), output_exit.size())\n            \n            ### Prenet\n            if is_inference:\n                prenet_output = self.prenet(output_mel)\n                # output exit is (1,1) sized cause BS=1\n                if output_exit.item() > 0.5:\n                    break\n            else:\n                prenet_output = self.prenet(true_mels[:, :, frame_num])\n            #print('5', prenet_output.size())\n            \n            \n            \n                    \n        \n        #### КОНЕЦ ЦИКЛА\n        \n        \n        \n        ### Postnet\n        output_mels = torch.stack(output_mels, dim=-1) ### (BS, 80, num_frames)\n        postnet_res = self.postnet(output_mels)   #(вроде сделано) эта свертка должна быть ПО ФРЕЙМАМ, каждый прогон цикла дает фрейм (1,80)  \n        post_output_mel = postnet_res + output_mels\n        \n        \n        \n        # exits\n        output_exits = torch.cat(output_exits, dim=1)\n        \n        return output_mels, output_exits, post_output_mel, att_mat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FullModel"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tacotron2(nn.Module):\n    def __init__(self, input_size):\n        super(Tacotron2, self).__init__()\n        \n        self.encoder = Encoder(input_size=input_size)\n        self.decoder = Decoder()\n        \n    \n    def forward(self, text_oheds, true_mels, text_lens, max_len):\n        \n        enc_outputs, _ = self.encoder(text_oheds, text_lens)\n\n        output_mels, output_exits, post_output_mel, att_mat = self.decoder(enc_outputs, true_mels, text_lens, max_len)\n        \n        return output_mels, output_exits, post_output_mel, att_mat\n    \n    \n    \n    # call with true_mels=None and max_len=MAX_EVER_LEN is OK\n    # BS=1\n    def inference(self, text_oheds, true_mels, text_lens, max_len):\n        \n        enc_outputs, _ = self.encoder(text_oheds, text_lens)\n        \n        output_mels, _, _, att_mat = self.decoder(enc_outputs, true_mels, text_lens, max_len, is_inference=True)\n        \n        return output_mels, att_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def W_loss_mat(dims, text_len, mel_len, g=0.2):\n    padded_T, padded_N = dims\n    N, T = mel_len, text_len\n    mat = torch.zeros((padded_N, padded_T))\n    for n in range(N):\n        t = np.arange(T)\n        mat[N-n-1, padded_T-T:] = torch.Tensor(1 - np.exp(- (n/N - t/T)**2 / (2*g**2)))\n    return mat.transpose(0, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RUNS"},{"metadata":{"trusted":true},"cell_type":"code","source":"tacotron = Tacotron2(input_size=my_dataset.chars_indexed.n_chars).to(device)\ncount_parameters(tacotron)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = torch.optim.Adam(tacotron.parameters(), weight_decay=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb\n\n!wandb login 6aa2251ef1ea5e572e6a7608c0152db29bd9a294\n\nwandb.init(project='dla-ht4')\nwandb.watch(tacotron)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tacotron.train()\n\nfor ep_num in range(0, 25):\n    \n    print('epoch num now will be', ep_num)\n    \n    for i, batch in tqdm(enumerate(train_loader)):\n\n        opt.zero_grad()\n\n        utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n        utt, text_ohed = utt.to(device), text_ohed.to(device)\n\n\n        true_mel_lens = get_mel_lens(utt_lens)                    # из-за сдвига\n        true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   ### ???\n        max_true_mel_len = true_mel_lens.max()\n        \n\n\n        # ~ masking\n        exit_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= true_mel_lens.unsqueeze(1)\n        exit_mask = exit_mask.to(device)\n        nullify_mask = ~exit_mask\n        nullify_mask = nullify_mask.repeat(80, 1, 1).transpose(0, 1)\n\n        true_mels = featurizer(utt)    \n        true_mels = true_mels[:, :, :-1]  # сдвиг для inputs модели\n        \n        output_mels, output_exits, post_output_mel, att_mat = tacotron(text_ohed, true_mels, text_ohed_lens, max_true_mel_len)\n\n        ### Losses\n        # умножить на маску и вход и выход, чтобы нули где пады были\n        output_mels = output_mels * nullify_mask\n        true_mels = true_mels * nullify_mask\n        L_pre  = F.mse_loss(output_mels, true_mels)             # true mel now\n\n\n\n\n        # masking for CE\n        # CE mask 1 less all than lens\n        ce_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= (true_mel_lens).unsqueeze(1)\n        ce_mask = ce_mask.to(device)\n        zeros = torch.zeros((utt.size(0), max_true_mel_len)).to(device)\n        exit_CE_ans = zeros.masked_fill(ce_mask, 1)\n\n        L_exit = F.binary_cross_entropy(output_exits, exit_CE_ans)   ####### true is exit or not\n\n\n        post_output_mel = post_output_mel * nullify_mask\n        L_post = F.mse_loss(post_output_mel, true_mels)\n\n        # mask att_mat and W_loss\n        # use: text_ohed_lens for 1 and true_mel_lens for 2\n                        \n        W = torch.zeros_like(att_mat)\n        for b_el_num, mat in enumerate(att_mat):\n            # lens == number of True\n            # for every column\n            \n            first = (torch.arange((mat.size(0))).repeat(mat.size(1), 1) >= text_ohed_lens[b_el_num]).transpose(0, 1)\n            first = ~first  # True = leave element not, False = delete\n    \n            second = (torch.arange((mat.size(1))).repeat(mat.size(0), 1) >= true_mel_lens[b_el_num])\n            second = ~second\n            act_mask_now = (first*second).to(device)\n            att_mat[b_el_num] = att_mat[b_el_num] * act_mask_now\n                        \n            # return 2d of needed size but good\n            W_b_el = W_loss_mat(act_mask_now.size(), text_ohed_lens[b_el_num], true_mel_lens[b_el_num].item())  \n            W[b_el_num] = W_b_el\n        \n        \n        W = W.to(device)\n        \n        L_guided = (att_mat*W).sum(dim=1).sum(dim=1).mean()\n\n        loss = L_pre + L_exit + L_post + L_guided        \n\n        wandb.log({'train_loss':loss,\n                   'pre':L_pre.item(), 'exit':L_exit.item(), 'post':L_post.item(), 'guided':L_guided.item()})\n\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(tacotron.parameters(), 10)\n\n        opt.step()\n        \n    torch.save({\n        'model_state_dict': tacotron.state_dict(),\n    }, '../../working/taco_new'+str(ep_num))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''asdnajdkasndajklsdnsakjld'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''checkpoint = torch.load('taco4', map_location=device)\ntacotron.load_state_dict(checkpoint['model_state_dict'])'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''# tacotron.train()\n# make EVAL\n    \n\nfor i, batch in tqdm(enumerate(val_loader)):\n\n    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n    utt, text_ohed = utt.to(device), text_ohed.to(device)\n\n\n    true_mel_lens = get_mel_lens(utt_lens)                    # из-за сдвига\n    true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   ### ???\n    max_true_mel_len = true_mel_lens.max()\n\n\n    # ~ masking\n    exit_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= true_mel_lens.unsqueeze(1)\n    exit_mask = exit_mask.to(device)\n    nullify_mask = ~exit_mask\n    nullify_mask = nullify_mask.repeat(80, 1, 1).transpose(0, 1)\n\n    true_mels = featurizer(utt)    \n    true_mels = true_mels[:, :, :-1]  # сдвиг для inputs модели\n\n\n    output_mels, output_exits, post_output_mel, att_mat = tacotron(text_ohed, true_mels, text_ohed_lens, max_true_mel_len)\n\n    \n    ### att_mat visualized\n    \n    for res in att_mat:\n        plt.figure(figsize=(15,2))\n        plt.pcolormesh(res)\n        #plt.xlabel('Embedding Dimensions')\n        #plt.xlim((0, dimensions))\n        #plt.ylim((20, 0))\n        #plt.ylabel('Token Position')\n        plt.colorbar()\n        plt.show()\n    \n    \n    ### Losses\n    # умножить на маску и вход и выход, чтобы нули где пады были\n    output_mels = output_mels * nullify_mask\n    true_mels = true_mels * nullify_mask\n    L_pre  = F.mse_loss(output_mels, true_mels)             # true mel now\n\n\n\n    # masking for CE\n    # CE mask 1 less all than lens\n    ce_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= (true_mel_lens).unsqueeze(1)\n    ce_mask = ce_mask.to(device)\n    zeros = torch.zeros((utt.size(0), max_true_mel_len)).to(device)\n    exit_CE_ans = zeros.masked_fill(ce_mask, 1)\n\n    L_exit = F.binary_cross_entropy(output_exits, exit_CE_ans)   ####### true is exit or not\n\n\n    post_output_mel = post_output_mel * nullify_mask\n    L_post = F.mse_loss(post_output_mel, true_mels)\n\n    L_guided = (att_mat*W_loss_mat(att_mat.size()).to(device)).sum(dim=1).sum(dim=1).mean()\n\n    loss = L_pre + L_exit + L_post + L_guided        \n\n    wandb.log({'val_loss':loss,\n               'pre_val':L_pre.item(), 'exit_val':L_exit.item(), 'post_val':L_post.item(), 'guided_val':L_guided.item()})\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''res = W_loss_mat(torch.rand((3, 20, 141)).size())\n\nprint(res.size())\n\nplt.figure(figsize=(15,2))\nplt.pcolormesh(res[2])\n#plt.xlabel('Embedding Dimensions')\n#plt.xlim((0, dimensions))\n#plt.ylim((20, 0))\n#plt.ylabel('Token Position')\nplt.colorbar()\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''for i, batch in tqdm(enumerate(train_loader)):\n    \n    \n\n\n    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n    true_mel_lens = get_mel_lens(utts_lens)\n    max_true_mel_len = max(true_mel_lens)\n    true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long)\n    \n    break'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''text_ohed = text_ohed.repeat(2, 1, 1)\ntext_ohed.size()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''text_ohed_lens'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''a = torch.nn.utils.rnn.pack_padded_sequence(text_ohed, lengths=text_ohed_lens, batch_first=True, enforce_sorted=False)\na'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''torch.nn.utils.rnn.pad_packed_sequence(a, batch_first=True, padding_value=75)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''np.sum(text_ohed_lens)'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}