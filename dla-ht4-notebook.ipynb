{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework №4\n",
    "This homework will be dedicated to the Text-to-Speech(**TTS**), specifically the acoustic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/dl-ht2/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(21)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(wav, sr, text):\n",
    "    print(text)\n",
    "    display.display(display.Audio(wav, rate=sr, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#win_len=1024, hop_len=256\n",
    "def get_mel_lens(audio_lens):\n",
    "    mel_lens = []\n",
    "    for len_now in audio_lens:\n",
    "        mel_lens.append(int((len_now - 1024)/256) + (1024//256)+1)\n",
    "    return mel_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this homework we will use only LJSpeech https://keithito.com/LJ-Speech-Dataset/.\n",
    "    Use text from `Normalized Transcription` field in transcripts.csv.\n",
    "    \n",
    "    Use the following `featurizer` (his configuration is +- standard for this task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MelSpectrogramConfig:\n",
    "    sr: int = 22050\n",
    "    win_length: int = 1024\n",
    "    hop_length: int = 256\n",
    "    n_fft: int = 1024\n",
    "    f_min: int = 0\n",
    "    f_max: int = 8000\n",
    "    n_mels: int = 80\n",
    "    power: float = 1.0\n",
    "        \n",
    "    # value of melspectrograms if we fed a silence into `MelSpectrogram`\n",
    "    pad_value: float = -11.5129251\n",
    "\n",
    "\n",
    "class MelSpectrogram(nn.Module):\n",
    "\n",
    "    def __init__(self, config: MelSpectrogramConfig):\n",
    "        super(MelSpectrogram, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=config.sr,\n",
    "            win_length=config.win_length,\n",
    "            hop_length=config.hop_length,\n",
    "            n_fft=config.n_fft,\n",
    "            f_min=config.f_min,\n",
    "            f_max=config.f_max,\n",
    "            n_mels=config.n_mels\n",
    "        )\n",
    "\n",
    "        # The is no way to set power in constructor in 0.5.0 version.\n",
    "        self.mel_spectrogram.spectrogram.power = config.power\n",
    "\n",
    "        # Default `torchaudio` mel basis uses HTK formula. In order to be compatible with WaveGlow\n",
    "        # we decided to use Slaney one instead (as well as `librosa` does by default).\n",
    "        mel_basis = librosa.filters.mel(\n",
    "            sr=config.sr,\n",
    "            n_fft=config.n_fft,\n",
    "            n_mels=config.n_mels,\n",
    "            fmin=config.f_min,\n",
    "            fmax=config.f_max\n",
    "        ).T\n",
    "        self.mel_spectrogram.mel_scale.fb.copy_(torch.tensor(mel_basis))\n",
    "    \n",
    "\n",
    "    def forward(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param audio: Expected shape is [B, T]\n",
    "        :return: Shape is [B, n_mels, T']\n",
    "        \"\"\"\n",
    "        \n",
    "        mel = self.mel_spectrogram(audio) \\\n",
    "            .clamp_(min=1e-5) \\\n",
    "            .log_()\n",
    "\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = MelSpectrogram(MelSpectrogramConfig()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../week01/audio.wav not found or is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c6c9ecb55d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../week01/audio.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torchaudio/backend/sox_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# check if valid file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found or is a directory\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# initialize output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ../week01/audio.wav not found or is a directory"
     ]
    }
   ],
   "source": [
    "wav, sr = torchaudio.load('../week01/audio.wav')\n",
    "wav = wav.to(device)\n",
    "mels = featurizer(wav).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 1, figsize=(15, 7))\n",
    "axes[0].plot(wav.squeeze())\n",
    "axes[1].imshow(mels.squeeze())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use pretrained Vocoder from Nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'waveglow'...\n",
      "remote: Enumerating objects: 190, done.\u001b[K\n",
      "remote: Total 190 (delta 0), reused 0 (delta 0), pack-reused 190\u001b[K\n",
      "Receiving objects: 100% (190/190), 435.59 KiB | 335.00 KiB/s, done.\n",
      "Resolving deltas: 100% (106/106), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/waveglow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  dla-ht4-notebook.ipynb  \u001b[0m\u001b[01;34mwaveglow\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Download pretrained model from https://github.com/NVIDIA/waveglow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in /anaconda/lib/python3.7/site-packages (0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdd.download_file_from_google_drive(\n",
    "    file_id='1rpK8CzAAirq9sWZhe9nlfvxMF1dRgFbF',\n",
    "    dest_path='./waveglow_256channels_universal_v5.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1A-APp73B0HkFGFcvUUCfswSBAJZSbf6F into ./new_dir... Done.\n"
     ]
    }
   ],
   "source": [
    "gdd.download_file_from_google_drive(\n",
    "    file_id='1A-APp73B0HkFGFcvUUCfswSBAJZSbf6F',\n",
    "    dest_path='./new_dir'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the following Vocoder for final quality validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Vocoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Vocoder, self).__init__()\n",
    "        \n",
    "        model = torch.load('waveglow_256channels_universal_v5.pt', map_location='cpu')['model']\n",
    "        self.net = model.remove_weightnorm(model)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, spect: torch.Tensor):\n",
    "        spect = self.net.upsample(spect)\n",
    "        \n",
    "        # trim the conv artifacts\n",
    "        time_cutoff = self.net.upsample.kernel_size[0] - self.net.upsample.stride[0]\n",
    "        spect = spect[:, :, :-time_cutoff]\n",
    "        \n",
    "        spect = spect.unfold(2, self.net.n_group, self.net.n_group) \\\n",
    "            .permute(0, 2, 1, 3) \\\n",
    "            .contiguous() \\\n",
    "            .flatten(start_dim=2) \\\n",
    "            .transpose(-1, -2)\n",
    "        \n",
    "        # generate prior\n",
    "        audio = torch.randn(spect.size(0), self.net.n_remaining_channels, spect.size(-1)) \\\n",
    "            .to(spect.device)\n",
    "        \n",
    "        for k in reversed(range(self.net.n_flows)):\n",
    "            n_half = int(audio.size(1) / 2)\n",
    "            audio_0 = audio[:, :n_half, :]\n",
    "            audio_1 = audio[:, n_half:, :]\n",
    "\n",
    "            output = self.net.WN[k]((audio_0, spect))\n",
    "\n",
    "            s = output[:, n_half:, :]\n",
    "            b = output[:, :n_half, :]\n",
    "            audio_1 = (audio_1 - b) / torch.exp(s)\n",
    "            audio = torch.cat([audio_0, audio_1], 1)\n",
    "\n",
    "            audio = self.net.convinv[k](audio, reverse=True)\n",
    "\n",
    "            if k % self.net.n_early_every == 0 and k > 0:\n",
    "                z = torch.randn(spect.size(0), self.net.n_early_size, spect.size(2))\n",
    "                audio = torch.cat((z, audio), 1)\n",
    "\n",
    "        audio = audio.permute(0, 2, 1) \\\n",
    "            .contiguous() \\\n",
    "            .view(audio.size(0), -1)\n",
    "        \n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder = Vocoder().to(device)\n",
    "vocoder = vocoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pretrained vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../week01/audio.wav not found or is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c6eb8e3d325f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../week01/audio.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreconstructed_wav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/dl-ht2/lib/python3.7/site-packages/torchaudio/backend/sox_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# check if valid file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found or is a directory\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# initialize output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ../week01/audio.wav not found or is a directory"
     ]
    }
   ],
   "source": [
    "wav, sr = torchaudio.load('../week01/audio.wav')\n",
    "wav = wav.to(device)\n",
    "mels = featurizer(wav).to(device)\n",
    "\n",
    "reconstructed_wav = vocoder.inference(mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reconstructed_wav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9c55496828aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_wav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reconstructed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstructed_wav' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(reconstructed_wav.squeeze(), label='reconstructed', alpha=.5)\n",
    "plt.plot(wav.squeeze(), label='GT', alpha=.5)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reconstructed_wav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3634e3a2512a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_wav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22050\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstructed_wav' is not defined"
     ]
    }
   ],
   "source": [
    "display.Audio(reconstructed_wav.squeeze(), rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c76ba207c5cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22050\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wav' is not defined"
     ]
    }
   ],
   "source": [
    "display.Audio(wav.squeeze(), rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) You need to implement Tacotron 2: https://www.dropbox.com/s/il9ai3ihh3nbcoo/Tacotron2.pdf?dl=0\n",
    "    \n",
    "    2) For fast convergence, implement Guided Attention: https://www.dropbox.com/s/tiibpqcdb1eieg4/GaidedAttention.pdf?dl=0\n",
    "       It will increase convergence by several times!\n",
    "    \n",
    "    3) (Bonus) Implement Monotonic Attention: https://www.dropbox.com/s/6deupekgd4ep0kg/MonotonicAttention.pdf?dl=0\n",
    "    \n",
    "    4) (Bonus) Implement Tacotron with GST.\n",
    "        For testing GST find 5-10 audio references with questioning tone and generate audio with this references.\n",
    "        \n",
    "    5) (PAY ATTENTION) Tacotron has RNNs, so don't forget about length masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) In this homework you are allowed to use pytorch-lighting.\n",
    "\n",
    "    2) Try to write code more structurally and cleanly!\n",
    "\n",
    "    3) Good logging of experiments save your nerves and time, so we ask you to use W&B.\n",
    "       Log loss, generated and real melspectrograms (in pair, i.e. real melspec and generated from correspond text). \n",
    "       Do not remove the logs until we have checked your work and given you a grade!\n",
    "\n",
    "    4) (Bonus) We also ask you to organize your code in github repo with Docker and setup.py. You can use my template https://github.com/markovka17/dl-start-pack.\n",
    "\n",
    "    5) Your work must be reproducable, so fix seed, save the weights of model, and etc.\n",
    "\n",
    "    6) In the end of your work write inference utils. Anyone should be able to take your weight, load it into the model and run it on some melspec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Finally, you need to write a report in W&B https://www.wandb.com/reports. Add examples of generated mel and audio, compare with GT.\n",
    "    Don't forget to add link to your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mLJSpeech-1.1\u001b[0m/  dla-ht4-notebook.ipynb  \u001b[01;34mwaveglow\u001b[0m/\r\n",
      "README.md      \u001b[01;34mwandb\u001b[0m/                  waveglow_256channels_universal_v5.pt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use text from `Normalized Transcription` field in transcripts.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharsInd:\n",
    "    def __init__(self):\n",
    "        self.char2ind = {}\n",
    "        self.char2count = {}\n",
    "        self.ind2char = {}\n",
    "        self.n_chars = 1     # PAD\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for char in sentence:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2ind:\n",
    "            self.char2ind[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.ind2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset containing text and audio.\"\"\"\n",
    "\n",
    "    def __init__(self, root='LJSpeech-1.1/', csv_path='metadata.csv', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root (string): Directory with all the data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root = root\n",
    "        self.csv = pd.read_csv(root+csv_path, sep='|', header=None)\n",
    "        self.csv = self.csv.drop(columns=[1]).rename(columns={0:'filename', 2:'norm_text'})  # leave only normilized\n",
    "        self.csv = self.csv.dropna().reset_index()\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.chars_indexed = CharsInd()\n",
    "        for i in range(self.csv.shape[0]):\n",
    "            self.chars_indexed.addSentence(self.csv.loc[i, 'norm_text'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt_name = self.root + 'wavs/' + self.csv.loc[idx, 'filename'] + '.wav'\n",
    "        utt = torchaudio.load(utt_name)[0].squeeze()\n",
    "        norm_text = self.csv.loc[idx, 'norm_text']\n",
    "        \n",
    "        text_ohed = torch.Tensor(\n",
    "                            [self.chars_indexed.char2ind[char_now] for char_now in norm_text]\n",
    "                    ).type(torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            utt = self.transform(utt)\n",
    "\n",
    "        sample = {'utt': utt, 'norm_text': norm_text, 'text_ohed':text_ohed}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "    wavs, wavs_lens = [], []\n",
    "    texts = []\n",
    "    texts_ohed, texts_ohed_lens = [], []    \n",
    "        \n",
    "    for el in data:\n",
    "        wavs.append(el['utt'])\n",
    "        wavs_lens.append(len(el['utt']))\n",
    "        texts.append(el['norm_text'])\n",
    "        texts_ohed.append(el['text_ohed'])\n",
    "        texts_ohed_lens.append(len(el['text_ohed']))\n",
    "    \n",
    "    wavs_padded = torch.nn.utils.rnn.pad_sequence(wavs, batch_first=True)\n",
    "    texts_ohed_padded = torch.nn.utils.rnn.pad_sequence(texts_ohed, batch_first=True, padding_value=75)\n",
    "    \n",
    "    return wavs_padded, wavs_lens, texts_ohed_padded, texts_ohed_lens, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all train+val samples: 13084\n"
     ]
    }
   ],
   "source": [
    "my_dataset = TextAudioDataset(csv_path='metadata.csv', transform=None)\n",
    "my_dataset_size = len(my_dataset)\n",
    "print('all train+val samples:', my_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(my_dataset_size * 0.85)\n",
    "val_len = my_dataset_size - train_len\n",
    "train_set, val_set = torch.utils.data.random_split(my_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, \n",
    "                          shuffle=False, collate_fn=my_collate_fn,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, collate_fn=my_collate_fn, \n",
    "                        num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loaders check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i, batch in enumerate(train_loader):\\n    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\\n    \\n    print(utt.size(), utt_lens)\\n    print(text_ohed.size(), text_ohed_lens)\\n    \\n    viz(utts[0], 22050, text[0])\\n    if i == 2:\\n        break'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i, batch in enumerate(train_loader):\n",
    "    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n",
    "    \n",
    "    print(utt.size(), utt_lens)\n",
    "    print(text_ohed.size(), text_ohed_lens)\n",
    "    \n",
    "    viz(utts[0], 22050, text[0])\n",
    "    if i == 2:\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enc = Encoder(input_size=my_dataset.chars_indexed.n_chars)\\nfor i, batch in enumerate(train_loader):\\n        \\n    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\\n    \\n    true_mel_lens = get_mel_lens(utts_lens)                   # из-за сдвига\\n    true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   ### ???\\n    max_true_mel_len = true_mel_lens.max()\\n        \\n        \\n    res, _ = enc(text_ohed, text_ohed_lens)\\n    print(res.size())\\n    \\n    viz(utt[0], 22050, text[0])\\n    if i == 2:\\n        break'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''enc = Encoder(input_size=my_dataset.chars_indexed.n_chars)\n",
    "for i, batch in enumerate(train_loader):\n",
    "        \n",
    "    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n",
    "    \n",
    "    true_mel_lens = get_mel_lens(utts_lens)                   # из-за сдвига\n",
    "    true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   ### ???\n",
    "    max_true_mel_len = true_mel_lens.max()\n",
    "        \n",
    "        \n",
    "    res, _ = enc(text_ohed, text_ohed_lens)\n",
    "    print(res.size())\n",
    "    \n",
    "    viz(utt[0], 22050, text[0])\n",
    "    if i == 2:\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Tacotron2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn_act(nn.Module):\n",
    "    def __init__(self, in_channels=512, out_channels=512, kernel_s=5, act=nn.Identity(), d_rate=0.5):\n",
    "        super(conv_bn_act, self).__init__()\n",
    "        \n",
    "        # batched 2d input, but first dim is considered channels, so kernel_size is actually (in_channels, kernel_s) \n",
    "        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n",
    "                              kernel_size=kernel_s, padding=int((kernel_s - 1)/2))\n",
    "        self.b_norm = nn.BatchNorm1d(out_channels)\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(d_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.dropout(self.act(\n",
    "                    self.b_norm(self.conv(inputs))\n",
    "               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size=512, lstm_hidden_size=256):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedder = nn.Embedding(input_size, emb_size, padding_idx=75)\n",
    "        self.convs = nn.ModuleList([conv_bn_act(in_channels=emb_size, out_channels=emb_size, act=nn.ReLU())\n",
    "                                    for i in range(3)])\n",
    "        self.bi_lstm = nn.LSTM(input_size=emb_size,                  # ???\n",
    "                               hidden_size=lstm_hidden_size,\n",
    "                               batch_first=True,                     # ???\n",
    "                               bidirectional=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, true_lens):\n",
    "        # (BS, seq_len)\n",
    "        x = self.embedder(inputs).transpose(1, 2)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        # (BS, seq_len, emb_size)\n",
    "        \n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=true_lens, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        encoded, _ = self.bi_lstm(x)\n",
    "        \n",
    "        encoded_unpacked = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        \n",
    "        return encoded_unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LS-attention COPIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationBlock(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_n_filters,\n",
    "        attention_kernel_size,\n",
    "        attention_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        padding = int((attention_kernel_size - 1) / 2)\n",
    "        self.conv = nn.Conv1d(\n",
    "            2, attention_n_filters, kernel_size=attention_kernel_size,\n",
    "            padding=padding, bias=False\n",
    "        )\n",
    "        self.projection = nn.Linear(attention_n_filters, attention_dim, bias=False)\n",
    "    \n",
    "    def forward(self, attention_weights):\n",
    "        output = self.conv(attention_weights).transpose(1, 2)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationSensitiveAttention(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        lstm_hidden_size,\n",
    "        attention_dim,\n",
    "        attention_location_n_filters,\n",
    "        attention_location_kernel_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_layer = nn.Linear(lstm_hidden_size, attention_dim, bias=False)\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.location_layer = LocationBlock(\n",
    "            attention_location_n_filters,\n",
    "            attention_location_kernel_size,\n",
    "            attention_dim\n",
    "        )\n",
    "        self.score_mask_value = -float(\"inf\")\n",
    "        \n",
    "    def get_alignment_energies(\n",
    "        self,\n",
    "        fir_lstm_hidden,\n",
    "        processed_memory,\n",
    "        alphas_concat\n",
    "    ):\n",
    "        \"\"\"\n",
    "        fir_lstm_hidden: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
    "        processed_memory: processed encoder outputs (B, ???, attention_dim)\n",
    "        alphas_concat: cumulative and prev. att weights (B, 2, max_time)\n",
    "        \"\"\"\n",
    "        processed_query = self.query_layer(fir_lstm_hidden.unsqueeze(1))\n",
    "        processed_alphas_concat = self.location_layer(alphas_concat)\n",
    "        \n",
    "        #print('LS-attention', processed_query.size(), processed_alphas_concat.size(), processed_memory.size())\n",
    "\n",
    "        energies = self.v(torch.tanh(\n",
    "            processed_query + processed_alphas_concat + processed_memory\n",
    "        ))\n",
    "\n",
    "        energies = energies.squeeze(2)\n",
    "        return energies\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        enc_outputs,\n",
    "        processed_memory,\n",
    "        fir_lstm_hidden,\n",
    "        alphas_concat,\n",
    "        mask=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        enc_outputs: encoder outputs\n",
    "        processed_memory: processed encoder outputs\n",
    "        fir_lstm_hidden: attention(first) rnn last output\n",
    "        alphas_concat: previous and cummulative attention weights\n",
    "        mask: binary mask for padded data\n",
    "        \"\"\"\n",
    "        \n",
    "        alignment = self.get_alignment_energies(\n",
    "            fir_lstm_hidden, processed_memory, alphas_concat\n",
    "        )\n",
    "\n",
    "        alignment = alignment.masked_fill(mask, self.score_mask_value) if mask is not None else alignment\n",
    "\n",
    "        attention_weights = F.softmax(alignment, dim=1)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), enc_outputs)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prenet_layer(nn.Module):\n",
    "    \"\"\"1 layer of (Linear, Relu, Dropout). 2 of this for real prenet layer in Tacotron2\"\"\"\n",
    "    \n",
    "    def __init__(self, in_size=256, out_size=256, d_rate=0.5):\n",
    "        super(prenet_layer, self).__init__()\n",
    "        \n",
    "        self.proj = nn.Linear(in_features=in_size, out_features=out_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(d_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.dropout(self.relu(self.proj(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, prenet_size=256, att_size=512, lstm_hid_size=1024, after_lstm_d_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.prenet = nn.Sequential(prenet_layer(in_size=80, out_size=prenet_size), \n",
    "                                    prenet_layer(in_size=prenet_size, out_size=prenet_size))\n",
    "        self.fir_LSTMCell = nn.LSTMCell(input_size=prenet_size+att_size, hidden_size=lstm_hid_size)\n",
    "        self.fir_dropout  = nn.Dropout(after_lstm_d_rate)\n",
    "        \n",
    "        ### 128 is attention hidden kinda\n",
    "        self.proj_memory  = nn.Linear(prenet_size*2, 128)\n",
    "        self.LS_attention = LocationSensitiveAttention(lstm_hid_size, 128, 32, 31)\n",
    "        \n",
    "        self.sec_LSTMCell = nn.LSTMCell(input_size=lstm_hid_size+att_size, hidden_size=lstm_hid_size)\n",
    "        self.sec_dropout  = nn.Dropout(after_lstm_d_rate)\n",
    "        self.proj_mel     = nn.Linear(lstm_hid_size+att_size, 80)\n",
    "        self.proj_exit    = nn.Linear(lstm_hid_size+att_size, 1)\n",
    "        \n",
    "        postnet_list = [conv_bn_act(in_channels=80, out_channels=512, act=nn.Tanh())]\n",
    "        for i in range(5-2):\n",
    "            postnet_list.append(\n",
    "                conv_bn_act(in_channels=512, out_channels=512, act=nn.Tanh())\n",
    "            )\n",
    "        postnet_list.append(conv_bn_act(in_channels=512, out_channels=80, act=nn.Identity()))\n",
    "        self.postnet = nn.Sequential(*postnet_list)   # * just unpacks list \n",
    "        \n",
    "       \n",
    "    \n",
    "    def forward(self, enc_outputs, true_mels, true_lens, max_len, is_inference=False):\n",
    "        BS = enc_outputs.size(0)\n",
    "        seq_len = enc_outputs.size(1)\n",
    "        processed_memory = self.proj_memory(enc_outputs)\n",
    "        # create mask from true_lens for chars: True <=> PAD, not use\n",
    "        true_lens = torch.Tensor(true_lens).type(torch.long)\n",
    "        att_mask = torch.arange(max(true_lens)).repeat(BS, 1) >= true_lens.unsqueeze(1)\n",
    "        att_mask = att_mask.to(enc_outputs.device)\n",
    "        #print('att mask size', att_mask.size())\n",
    "        #print('enc outputs in att size', enc_outputs.size())\n",
    "        \n",
    "        device = enc_outputs.device\n",
    "        \n",
    "        prenet_output = torch.zeros((BS, 256), device=device)         # всегда?\n",
    "        att_context    = torch.zeros((BS, 512), device=device)\n",
    "        fir_lstm_hidden = torch.zeros((BS, 1024), device=device)\n",
    "        fir_lstm_context = torch.zeros((BS, 1024), device=device)\n",
    "        alphas         = torch.zeros((BS, seq_len), device=device)\n",
    "        alphas_sum     = torch.zeros((BS, seq_len), device=device)\n",
    "        att_mat        = torch.zeros((BS, seq_len, 1), device=device)\n",
    "        sec_lstm_hidden = torch.zeros((BS, 1024), device=device)\n",
    "        sec_lstm_context = torch.zeros((BS, 1024), device=device)\n",
    "        \n",
    "        output_exits, output_mels = [], []\n",
    "        for frame_num in range(max_len):\n",
    "            \n",
    "            ### concat + 1_lstm + 1_dropout\n",
    "            fir_lstm_inputs = torch.cat((prenet_output, att_context), dim=1)  # в длинну 256+512\n",
    "            fir_lstm_hidden, fir_lstm_context = self.fir_LSTMCell(fir_lstm_inputs, (fir_lstm_hidden, fir_lstm_context))\n",
    "            fir_lstm_hidden = self.fir_dropout(fir_lstm_hidden)\n",
    "            #print('1', fir_lstm_hidden.size(), fir_lstm_context.size())\n",
    "\n",
    "\n",
    "            ### concat + LS_att\n",
    "            alphas_concat = torch.stack((alphas, alphas_sum), dim=-1).transpose(1, 2)  # по новому дименшну\n",
    "            \n",
    "            att_context, alphas = self.LS_attention(enc_outputs, processed_memory, fir_lstm_hidden, alphas_concat,\n",
    "                                                    mask=att_mask)\n",
    "            alphas_sum = alphas_sum + alphas\n",
    "            att_mat = torch.cat((att_mat, alphas.unsqueeze(2)), dim=2)              #### CHECK\n",
    "            #print('2', att_context.size(), alphas_concat.size())\n",
    "\n",
    "\n",
    "            ### concat + 2_lstm\n",
    "            sec_lstm_inputs = torch.cat((fir_lstm_hidden, att_context), dim=1)  # в длину 1024+512\n",
    "            sec_lstm_hidden, sec_lstm_context = self.sec_LSTMCell(sec_lstm_inputs, (sec_lstm_hidden, sec_lstm_context))\n",
    "            sec_lstm_hidden = self.sec_dropout(sec_lstm_hidden)\n",
    "            #print('3', sec_lstm_hidden.size(), sec_lstm_context.size())\n",
    "\n",
    "            ### concat + FC x 2\n",
    "            FC_inputs = torch.cat((sec_lstm_hidden, att_context), dim=1)   # в длину?\n",
    "            output_mel = self.proj_mel(FC_inputs)\n",
    "            output_mels.append(output_mel)\n",
    "            output_exit = F.sigmoid(self.proj_exit(FC_inputs))\n",
    "            output_exits.append(output_exit)\n",
    "            #print('4', output_mel.size(), output_exit.size())\n",
    "            \n",
    "            ### Prenet\n",
    "            if is_inference:\n",
    "                prenet_output = self.prenet(output_mel)\n",
    "                # output exit is (1,1) sized cause BS=1\n",
    "                if output_exit.item() > 0.5:\n",
    "                    break\n",
    "            else:\n",
    "                prenet_output = self.prenet(true_mels[:, :, frame_num])\n",
    "            #print('5', prenet_output.size())\n",
    "            \n",
    "            \n",
    "            \n",
    "                    \n",
    "        \n",
    "        #### КОНЕЦ ЦИКЛА\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Postnet\n",
    "        output_mels = torch.stack(output_mels, dim=-1) ### (BS, 80, num_frames)\n",
    "        postnet_res = self.postnet(output_mels)   #(вроде сделано) эта свертка должна быть ПО ФРЕЙМАМ, каждый прогон цикла дает фрейм (1,80)  \n",
    "        post_output_mel = postnet_res + output_mels\n",
    "        \n",
    "        \n",
    "        \n",
    "        # exits\n",
    "        output_exits = torch.cat(output_exits, dim=1)\n",
    "        \n",
    "        return output_mels, output_exits, post_output_mel, att_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FullModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_size=input_size)\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    \n",
    "    def forward(self, text_oheds, true_mels, text_lens, max_len):\n",
    "        \n",
    "        enc_outputs, _ = self.encoder(text_oheds, text_lens)\n",
    "\n",
    "        output_mels, output_exits, post_output_mel, att_mat = self.decoder(enc_outputs, true_mels, text_lens, max_len)\n",
    "        \n",
    "        return output_mels, output_exits, post_output_mel, att_mat\n",
    "    \n",
    "    \n",
    "    \n",
    "    # call with true_mels=None and max_len=MAX_EVER_LEN is OK\n",
    "    # BS=1\n",
    "    def inference(self, text_oheds, true_mels, text_lens, max_len):\n",
    "        \n",
    "        enc_outputs, _ = self.encoder(text_oheds, text_lens)\n",
    "        \n",
    "        output_mels, _, _, att_mat = self.decoder(enc_outputs, true_mels, text_lens, max_len, is_inference=True)\n",
    "        \n",
    "        return output_mels, att_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_loss_mat(dims, g=0.2):\n",
    "    BS, N, T = dims\n",
    "    mat = torch.zeros((N, T))\n",
    "    for n in range(N):\n",
    "        for t in range(T):\n",
    "            mat[N-n-1, t] = 1 - np.exp(- (n/N - t/T)**2 / (2*g**2))\n",
    "    return mat.repeat(BS, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28156929"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tacotron = Tacotron2(input_size=my_dataset.chars_indexed.n_chars).to(device)\n",
    "count_parameters(tacotron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(tacotron.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkirili4ik\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">divine-tree-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kirili4ik/dla-ht4\" target=\"_blank\">https://wandb.ai/kirili4ik/dla-ht4</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kirili4ik/dla-ht4/runs/5tmfi0ud\" target=\"_blank\">https://wandb.ai/kirili4ik/dla-ht4/runs/5tmfi0ud</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201124_005609-5tmfi0ud</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7fe555ceda50>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "wandb.watch(tacotron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num now will be 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "696it [1:46:25,  9.17s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num now will be 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "696it [1:46:11,  9.16s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num now will be 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "696it [1:45:52,  9.13s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num now will be 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "696it [1:46:05,  9.15s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num now will be 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [10:08,  8.87s/it]"
     ]
    }
   ],
   "source": [
    "tacotron.train()\n",
    "\n",
    "for ep_num in range(15):\n",
    "    \n",
    "    print('epoch num now will be', ep_num)\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n",
    "        utt, text_ohed = utt.to(device), text_ohed.to(device)\n",
    "\n",
    "\n",
    "        true_mel_lens = get_mel_lens(utt_lens)                    # из-за сдвига\n",
    "        true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long) - 1   ### ???\n",
    "        max_true_mel_len = true_mel_lens.max()\n",
    "\n",
    "\n",
    "\n",
    "        # ~ masking\n",
    "        exit_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= true_mel_lens.unsqueeze(1)\n",
    "        exit_mask = exit_mask.to(device)\n",
    "        nullify_mask = ~exit_mask\n",
    "        nullify_mask = nullify_mask.repeat(80, 1, 1).transpose(0, 1)\n",
    "\n",
    "        true_mels = featurizer(utt)    \n",
    "        true_mels = true_mels[:, :, :-1]  # сдвиг для inputs модели\n",
    "\n",
    "\n",
    "        output_mels, output_exits, post_output_mel, att_mat = tacotron(text_ohed, true_mels, text_ohed_lens, max_true_mel_len)\n",
    "\n",
    "        ### Losses\n",
    "        # умножить на маску и вход и выход, чтобы нули где пады были\n",
    "        output_mels = output_mels * nullify_mask\n",
    "        true_mels = true_mels * nullify_mask\n",
    "        L_pre  = F.mse_loss(output_mels, true_mels)             # true mel now\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # masking for CE\n",
    "        # CE mask 1 less all than lens\n",
    "        ce_mask = torch.arange(max_true_mel_len).repeat(utt.size(0), 1) >= (true_mel_lens).unsqueeze(1)\n",
    "        ce_mask = ce_mask.to(device)\n",
    "        zeros = torch.zeros((utt.size(0), max_true_mel_len)).to(device)\n",
    "        exit_CE_ans = zeros.masked_fill(ce_mask, 1)\n",
    "\n",
    "        L_exit = F.binary_cross_entropy(output_exits, exit_CE_ans)   ####### true is exit or not\n",
    "\n",
    "\n",
    "        post_output_mel = post_output_mel * nullify_mask\n",
    "        L_post = F.mse_loss(post_output_mel, true_mels)\n",
    "\n",
    "        L_guided = (att_mat*W_loss_mat(att_mat.size()).to(device)).sum(dim=1).sum(dim=1).mean()\n",
    "\n",
    "        loss = L_pre + L_exit + L_post + L_guided        \n",
    "\n",
    "        wandb.log({'train_loss':loss,\n",
    "                   'pre':L_pre.item(), 'exit':L_exit.item(), 'post':L_post.item(), 'guided':L_guided.item()})\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(tacotron.parameters(), 10)\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "    torch.save({\n",
    "        'model_state_dict': tacotron.state_dict(),\n",
    "    }, 'taco'+str(ep_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20, 141])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwUAAACQCAYAAABZJscUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAn1UlEQVR4nO3de5CkV33e8e+vey6rFXfWgKwVSLGFE0XhFkXgwhXLBtvCobSkkhApdiwcYiVVUYJtnERASnZIpQriGJJUZCpro0hxERQFY7NlCwTBcil2DNYKgtDFsrYkQKsICV24FNqdvv3yx9srdZ/zm+mzZ6bnnZ1+PlVdO33mvGfeebdntt/tfp7X3B0REREREVlcnbZ3QERERERE2qWTAhERERGRBaeTAhERERGRBaeTAhERERGRBaeTAhERERGRBaeTAhERERGRBTfzpMDMzjKzW8zsbjO7y8zeMR5/gZl9xszuG//5/PnvroiIiIiIbDWbdZ0CMzsDOMPdv2BmzwZuB94CvA14wt3fZ2ZXAc9393855/0VEREREZEtNvOVAnd/2N2/MP74O8A9wJnAAeD68bTraU4URERERETkFDPzlYKpyWZnA7cC5wNfc/fnjccNePLEfREREREROXUslU40s2cBvw38vLt/uzkPaLi7m1l4dmFmVwBXAHTp/tW9PGdze7z+DuZDJfOC7UrXz9eq3K5wLQ+3m/01PZwTrN+ZvX60Vun6JWvF33fBvE3tlxfMKdiO6OEVzcnHwkOfzOtQulYwlmwbPZQ6jIL1Z8+Lvp4F+xruf/b1SrebPRb/aJSudfJz1hur+XrrDVrBVyj8jbZl28VrbeVq0frbv+Wp8xVF5ITb71h7zN2/p+392MhP/Mjp/vgTw2z89jvWbnb3i7dzX4pOCsxsmeaE4CPu/vHx8CNmdoa7PzzOHTwabevuB4GDAM+xF/hr7Q0nv5fWSe5G/1rm74Sybjef103WWl6eOQfAloJDlY4tF8xZZ54vT++rr+RzRsv59zNayffVl6fHhsv5nNFKfgyHwVrpvGFwuIbBWqNg3mgluR8cmnTOumstp/fzJ1rxdsG563LyZDias5Q/Ye6m2wHd5ekf7OWl/Ad9dXmQja1083krS9PzTlvqZ3P2dPO14rHpbVc7+ZzTur18X4N5e5N5q51gvyzfLpq3ksxbtWhOfmz2BGstJ2stExzTYK10O4Bu8sR9Odwu+PsPnvDna0UnPrlu8GuumzzFjLcrexraSdZK1y7dLlK+D3UFeCX7EOkG/07MW+33KCKb1z3jvq+2vQ+zPPbEgP/zqTOz8T3f+8C+7d6XkvYhAz4M3OPuH5j41CHg8vHHlwOf2PrdExERERHZnRwYMMxubSh5peD1wN8Hvmxm/3c89m7gfcCNZvZ24KvAW+eyhyIiIiIiu5Dj9D1/5bkNM08K3P2PWP9tkRXvBRIRERERkRFw3Nt5ZSBVHDQWEREREZGt4zj9II/Whm09KbBOh85pe58ZGAYvlwQvofhwmEwJDp7nwUMf5GPVoiBzGniuDDtH8ywIIy+VhJ0BlpKvGYWpC8LO0dgoCECnweZmXj6WBp6jsPMoWGu4GoSis6BxFJzOhsJ56dcsCUnDekHp6cfmsWCtp5byx6+vBD8LaeC5MuwMeeA5CkDvWQoC0MFYGnguDTuvhOHmfnK/LOwcB6DXpvchCBBHoeIotJwGnmvDzpAHnmvDzs28YcGcsgB02hq1HIWkC8LOEITTgl/RUfg4CgyXBJ5Lg8bp1xwE/xs377DzMGj12u7As8LOIjuXO/R3xjmBXikQEREREWmDYxz3nXHirpMCEREREZEWONDXSYGIiIiIyOIaYRz3nfF0fGfshYiIiIjIgnGMngf50xZs70nByjL2sv1P37V+Hq6jH4SDB8OZc3wQrBUEmT3dtjLs3ExLkiE7IewczKsNO0MeeK4OO0MeeK4MO0MeeK4NOzfzpo9hGHYOgsbxlZyTK0BXhp2bsXQ/g7WisHMQbj6WBKCjsPM3oys5RwHoJPBcGnZeiq7anMwrDTuvFoSbS67sDHnYuRlLr9pcFnYOQ8vJ2J7gqs0lYWfIQ8qlwemSqzt3gkBsHIrOx/KrNteFnZt9nR6rDjtDFnjeCWFnyAPPmwkCl+zHTgg7RxSAFhm/fYhFPCkQEREREREARm4c9+B/81qgkwIRERERkRY4Rl+ZAhERERGRxbW4mQIREREREQHG7UNRGLAFOikQEREREWlBc0XjnfF0fFv3YrRnie++/PlP3+/08kaETnCt504vaWroB9v18kYM688eq25AAkgbj4I5cStSMC8dK5lD0IAEWQvSTmhAgqDdqLIBCYIWpNoGJMhakEobkOJWpG5yv64BCfJGotImo7JWpGBO1GS0EnyPaStScEijseMr+WP1WNKCFDUgedBuZEvB74Bk3lLUgLQ0uwEJYCVpStqzHLQDFTQgRWOlDUhRu1FJK1JJAxLkLUhha1HQNBS1J6UtSKXbRe1GaQtSbQNSM280c05JAxLkLUi1DUjNWtPb1jYgrbftrK8HeQNSs1ZdG1BpE1OmcrOtpAYkaVuTKdgZbx/ST4OIiIiISAucpn0ovc1iZheb2b1mdsTMrgo+/1Izu8XMvmhmd5jZT85aUycFIiIiIiItOPFKQXrbiJl1gWuANwHnAZeZ2XnJtH8F3OjurwYuBX591r7opEBEREREpAUOJ31SAFwIHHH3+929B9wAHAiWfs744+cC/2/Wojsj2SAiIiIismBGbqydfPvQmcCDE/ePAq9N5vwK8Gkz+6fA6cAbZy26rScFg1V44uXPnP10+vmZUJC3I82/dXtBWCzYLp43PdZdKww7F4SbS8POBGOdNPBcG3YO5lWHnYN5tWHnZj/mF3iuDjtDFnguDTt3gnBzNwtAF4adg3lpkNlX8q9XEnZuxqa/x9FSWdh5GAWlk8Bzbdg5mlcbdo7GwrBzEGQ+FoabPblfF3aGPPBcG3aGPPBcG3ZuxqbXqg07R/Nqw87RtrVh52jblYI5AMsEfx/JvNqwczQvCjvHa2VDRYHkTkHYGfLA8CKEnYcEfz/Bvx3zpsDz4nKM/ih8ZWCfmR2euH/Q3Q+exNKXAde5+6+Z2Q8Cv2Vm57t7/qAfm3lSYGbXAm8GHnX388djvwL8HPCN8bR3u/tNJ7GjIiIiIiILbYP2ocfc/YJ1NnsIOGvi/v7x2KS3AxcDuPufmNkeYB/w6Hr7UnJqet2JRRMfdPdXjW86IRAREREROQnusDZaym4z3Aaca2bnmNkKTZD4UDLna8AbAMzsLwF7eOY/80Mzv6q732pmZ8+aJyIiIiIi5WquU+DuAzO7ErgZ6ALXuvtdZvZe4LC7HwLeCfyGmf0CTej4be4evJnwGZvJFFxpZj8DHAbe6e5PbmItEREREZGF4hi92a8M5Ns179K5KRm7euLju4HXn8yatcmWDwHfB7wKeBj4tfUmmtkVZnbYzA4Pj3238suJiIiIiOwu7tAfdbNbG6peKXD3R058bGa/AfzeBnMPAgcBVs/e79/9/on2h37QfjDIx6yXXBI+2i5sLZo9r9PLD3xQuEFQuJE3GQVzShqQALrJWKcXtR3NbkACsKQpqdMLvqGSBiTIG4/CVqSC7QhakCobkKJ5O6EBqRmy2XMKGpCieWEDUrBW1oAEkG5bMoe43aikFSlsDAobj5JWpKgBKWotWp3dbhRtF60ftiJlTUZRo1M2tE570vT9sAFpKWhAWolakaZ/tmsbkCBvQVoOWpFWo6akpaCRaGn656q2AQnyRqXToiajYK3TOvkv4LTJqLYBCfK2odoGJMhbkGobkKJtaxuQIG9BCudkI7ASNAtl21U2IDX7UbB+YftQ9DVTo43fYbHpfYikLUhqQFocjjHwnXHsq04KzOwMd394fPdvAndu3S6JiIiIiOx+7kZvuDMuG1ZSSfpR4CKavtSjwC8DF5nZq2iCC18B/tH8dlFEREREZPdxOHVeKXD3y4LhD89hX0REREREFoYDg9EpclIgIiIiIiJbz93otRQsTm3rScHyypAzXvb40/d7w/wg9Pr5LvUH0/MGg/yMqtcPApHBPO9Pj6UhZlgn7ByFltMAdJC3TefAOqHoJCNXG3YGSLN73bW6sHOzX2louS7s3IxNz7MgAF0UdoY88FwbdoYsyFwbdoYg8LwTws7BvNqwczTWCQLKRWFnyAPPlWFnyAPPo+Ug2BwGhqMAdF1oeVgQlC4NO0eh5WES4B4t53NKx44nQeYw7ByEon0l+NlOAs+lYeduN5+3sjz9M1oado7Cx3uLAtClQeZ+wZzZYWfIA89xaDn/vqO10m1rw87RvNqwM+Qh5dqwczQv+j/VbvBrLwoop4Hn3RZ2Ppn9SKVhZ9j+wPMihp31SoGIiIiIyIJzjKFOCkREREREFteJ6xTsBDopEBERERFphV4pEBERERFZaO4wHNVf+G4r6aRARERERKQFji3m24dOX1rjdS/6ytP314J6jWPDvCbjeDKW3m/G8rWOD/Kx9Kpx0Zy07QjiVqS0Bam2AQnydqNO0HYUNiCFYwVzwiaj2WNBUQed3uwGpGat6XlRK1JJA1I01onajsJWpGAsaSSK2pSIWpHSBiTIW5AqG5AgaDcqaTuKtoOsBWknNCBB0G5U2YAEeQtS2IAUtS6tBNU8yVrFDUhhu1HaGFTXgARlTUZRU1LUipQ2HoWtSAUNSM1aG99fb2ywkj9+jyctSGEDUtBuZMG8TtJ41F0KWpGidqPl/Gd0pTs9bzWYEzUShY1HSXtS1IB0WjQWNRkl668GTUZ7g+q6tAEJoiajsgakcJ71Zs4paUCCvAUpakCKWotWgvakdN5K0LhT0oAEeQtSbQNSs23yHKC0yShav2DbkgYkqG9BOpUbkNow2iGvFOz+Iy0iIiIisgO5w2DYyW6zmNnFZnavmR0xs6vWmfNWM7vbzO4ys/8+a029fUhEREREpAWOMTrJoLGZdYFrgB8DjgK3mdkhd797Ys65wLuA17v7k2b2olnr6pUCEREREZE2OIzcstsMFwJH3P1+d+8BNwAHkjk/B1zj7k8CuPujsxbVSYGIiIiISEtGQ8tuM5wJPDhx/+h4bNLLgZeb2R+b2efM7OJZi25v0Li7xl971gNP3z8eJM/6nofYjvv0vLVgu2itKMj81Ghl5pySsHM0Vht2jubVhp2bedPb1oadAWyQhJ96dWHnaF5t2BnyIHNt2Bmgm2yb3m/WD8JoQbg5nVcbdm7WSuZFYecotBwGoKfXCsPIwVrhvGSsNuwM8w08F4edo3lJINmC0HJJ2BmCwHNl2BnyIHNt2BnywHMaPG62y49XGFpOxmrDztFYHFrOv5/hSj7Pl2bP6QUB5eNhuDkNQBeGnYMgczfZtjbsDHnguTbs3Myb/nmsDTtDHnguDTvHoeXpebVh52hebdi52XZ6rDbs3MwbzZxTEnaGPPBcG3Zu1i8ILe/QsPOpwh08fvvQPjM7PHH/oLsfPImll4BzgYuA/cCtZvZX3P2bG20gIiIiIiIt8Pz/GAAec/cL1tnkIeCsifv7x2OTjgKfd/c+8ICZ/TnNScJt6+2H3j4kIiIiItIGN0bDTnab4TbgXDM7x8xWgEuBQ8mc36V5lQAz20fzdqL7N1pUJwUiIiIiIm1xy28bTXcfAFcCNwP3ADe6+11m9l4zu2Q87WbgcTO7G7gF+Ofu/vhG6+rtQyIiIiIibYnfPrQhd78JuCkZu3riYwd+cXwropMCEREREZE2OPjstqFtsa0nBas24NyVR56+3/eg2SIY6zPdNBE1FPXSmgny1qJm2+l5tQ1IkLcg1TYgRfOiBqResFbYipQ0GT01CNYKGpB6QePRWrJWaQNSPxgbpa1IlQ1IkLcg1TYgRfPCVqSg5Cco08jajUrmNPOCVolkXtSA1I1akYIxS9qHbK2yAQnyFqTKBiQIWooqG5Agb0EqbkDKZ9UraTeqbECCvAWpugEpWj9qQArW96DJyFem1y9tQPKlqN1oeixqQIpai4arwVhBk1F5K1J6v64BKZpX24DUjCWNZ5UNSJC3IC0H260GrUUrwbw9S0ljUGUDEuQtSCvBL+S9wXZpAxLAarJt1GQUtSKlDUjRtrUNSJC3INU2IEHeXFTbgBTNq21AarZN/t2ecwPSqcRGO2P/Z2YKzOxaM3vUzO6cGHuBmX3GzO4b//n8+e6miIiIiMgu4waj4NaCkqDxdUB6wYOrgM+6+7nAZ8f3RURERETkZAyDWwtmnhS4+63AE8nwAeD68cfXA2/Z2t0SEREREdnlvHn7UHprQ20l6Yvd/eHxx18HXrzeRDO7wswOm9nhbz7R0qmPiIiIiMhO5MGtBZsOGru7mwWJlWc+fxA4CPCKVy77S7prT39uGGwVtTINkwDJKNiuF5zfjIKe114SWg7nBEHjNOwMeSC5NuwcrRWFlkvCztG2Udh5LVjrqYJwcxSALgk7N/OSkHcUgC4IOwP0k3mDYLt+EGQeBfM8CR9bLwiCRmHnKLScBaCzKeWh6CSfVht2jtaPgs0lYedmv6Z/SmvDzpAHnqvDzpCHlKOgcRRkLlirNuwcbxtsN6joo1tPEGQOpyUh4uKwc0EAujrsDJDOC9YKw87B+r6alBssRQHo2WHnaF4Ydg5Cy3GQOb1fuF1wCNMAdxSmLh3rrUw/fqOw87eXgsf4SvDznmxbG3aGPPBcG3aGPPBcG3aGPPBcG3aGPLRcG3aGPKS8bMHXC9ZKw87hWpVhZ8gDz7VhZ8gDz6f0RbccbIe0D9Uex0fM7AyA8Z+Pbt0uiYiIiIgsBhvltzbUnhQcAi4ff3w58Imt2R0RERERkcVgDjbMb20oqST9KPAnwA+Y2VEzezvwPuDHzOw+4I3j+yIiIiIicjJ2SCXpzEyBu1+2zqfesMX7IiIiIiKyUNp6u1BqW69o3MV4rj2TbhoF4ZFhQeR6FMwZen5E49DydJCmNuwMeeC5NuwczasNOzfbLs2cc9zzS2+G89KrNleGnZt50/tVG3aGPPBcG3Zu5k1vWxt2hjzwXBt2BrBk2+hqz1FoOZqXBpKj4HQcWp49Fs2JQ8vBvPQK0Gt1YefmayZXdg3Cu1Fo2frBWG/690QYdo7Cx73gmyy5anMUdo6CzAVh6ijsTPD7MQ1Ab2nYORJdyTkKLaeB58qwM0AnnRcEjVkOErdRKDoJPNeGnZt5STi4MuwMeUi59ArN0dWXi9YqCDs3a218f72x8OrOSQA6DDsH21kQZO4sJ2HXpSjsnI+tLOc/o2ngOQo7rwRXct4bBaCTtaKwc3Ql5ygAnQaZ96atFcEcKLu6cxRQDq/kHAagk++xMuwMeeC5u1OeVdfw9t4ulDqlA9siIiIiIqcy8/w2cxuzi83sXjM7YmbrXkTYzP6WmbmZXTBrTZ0UiIiIiIi0wU++fcjMusA1wJuA84DLzOy8YN6zgXcAny/ZFZ0UiIiIiIi0wKhqH7oQOOLu97t7D7gBOBDM+zfA+4HjJfuikwIRERERkTZUvFIAnAk8OHH/6HjsaWb2GuAsd//90l3Z1qCxiIiIiIhMiE8C9pnZ4Yn7B939YMlyZtYBPgC87WR2Y1tPCjoYq/bMl4xahEqMoqMXVLoOvaDJqLIBqdmP6Xm1DUjNtiXbzW5AgrwFqbYBCfIWpNoGpGhebQNSs+30WLRd1Ir0VFC5kbYgrQUNRceCtdIGJMhbkEobkKKvudafHusNg+Pcz7dLG5Agb0GqbUBqxqbn1TYgQd6CVNuAFI2VtiKlDUgAnWRe2KYUNCCFTUlJu1FxA1Iwlq5F1FpU0oAEWQtSdQNSMK+2AalZf44tIrUNSJC1IFU3IEHeglTZgAR5C1LYgLQc/LxHjUFJu9FouawBKWopSsfCBqSokShaazldK9guXGv2vGi7ftBktLaSj6UtSKUNSBa0G3WTVqTSBqTloPEobTKKGpDSOQB7onlJu1HUilTSgAR5C1JtAxLkzUVRQ1HjgXXGd5D124cec/f1wsEPAWdN3N8/Hjvh2cD5wB+aGcBLgENmdom7T55oTNErBSIiIiIiLaloVL0NONfMzqE5GbgU+HsnPunu3wL2Pb2+2R8Cv7TRCQEoUyAiIiIi0pqTzRS4+wC4ErgZuAe40d3vMrP3mtkltfuhVwpERERERFpgXndFY3e/CbgpGbt6nbkXlaypkwIRERERkZbslAsyb/NJgdGdCHoF8asiwyAQG1kqmFYbdm62Tf4WK8POkAeea8POzdccJXOC/SoIO0fb1oadm3mWzKkLO0MeeK4NO0MeeC4NO8drJaHlyrAz5OHj2rAz5IHn2rAz5IHn4rDzIHhM9JOQZGXYGfLQcqeXz4lC0fHY9P0wAB3kbTsFQenasHM0L5oTB6CD0HISeK4OO0MeeK4MO0MQeI7CyNFaw4Ig804IO0MWeK4NO0fbRmt1otByGnaGPNwcBaALws6QB56Lw84F4eYw7BwFmVeDf6+SebVh52atje83awVjBUHpKOzcC8ZGwVgaeK4NO0MeeK4NO0MeeK4NO0djUdi58cfrjO8gla8UzINeKRARERERaUnBxcq2hU4KRERERETaoFcKREREREQWm6FXCkREREREFpuDReHMFuikQERERESkJQv59iEDOltwvbROcKn6rAmoUG0DEpS1IJU0IEF9C1L4fSdfs7YBKRK1IpU0IDXz0rXqGpCabZPL3gfbRU1JfQ9aK5J5cZPR7AakaNvSBqR+MC9tMqptQIK8BSla61hQkxE1EqUtSNGcsAEpmPfUYPZavUHQihS0G6UtSMNh/nc96AdtJ0G7kSdNSdara0CCvAWptgEJ8haksAEp2C5qRUqbjKJWpJIGpGhecQNSP2hF6U1/U9UNSJC3G5XMIWhAgqwFqboBCbIWpJ3QgNQMTT8OoyajkgakaKy6AQnyFqRgOw9bkYKxpAXJw7aj4PdE0HiUjsUNRWWtSOmvzHC7oBVpFByu9Fd5cStSsFZ/ZfpnO2xAWgoakFbysbQFqbYBCfLGo6gV6ZTh0NkNbx8ys68A3wGGwMDdL9iKnRIRERER2e2M3fVKwY+4+2NbsI6IiIiIyEJRpkBEREREZJE52A5599Nm3+DvwKfN7HYzu2IrdkhEREREZCGM24fSWxs2+0rBD7n7Q2b2IuAzZvZn7n7r5ITxycIVAC89c34vTGxFgPmkv2YS1KoNO0N94PlUCTtDWeC5JOwMeeA5DjtHoejZaZ6e50nNkrAz5IHn2rBzMy8NLdeFnZttl2bOKQk7R9tGoeWSsHO0bW3YOZpXGnaOxo4lAejSsHM/GBskY7VhZwDrpwHowrBzFGROA9BhSDobigPQ/dlzSsLOzbZJAHqtLuzcjE3/5HaC8G6nl/9OSMPOkAeeq8PO0bwo7BwFlIP1s3mVYedmaPoY+jD4S9tKBWFnCILMlWFnCALPtWHnYNso7MxyFFqOAtDJ84nKsDPkgefS0PIwCh8n29aGnZu1Nr6/3lg/CDf3kiDzd4Kw86lkp2QKNvVM2t0fGv/5KPA7wIXBnIPufoG7X/A9L9xM14+IiIiIyO5h7nSG+W3mdmYXm9m9ZnbEzK4KPv+LZna3md1hZp81s5fNWrP6pMDMTjezZ5/4GPhx4M7a9UREREREFo2N8tuG8826wDXAm4DzgMvM7Lxk2heBC9z9FcDHgH83az8280rBi4E/MrMvAX8K/L67f2oT64mIiIiILA6nuUhTetvYhcARd7/f3XvADcCBqWXdb3H3p8Z3Pwfsn7Vo9Zv83f1+4JW124uIiIiILLqStwslzgQenLh/FHjtBvPfDnxy1qKqJBURERERaYH5utcp2GdmhyfuH3T3gye9vtlPAxcAPzxrrk4KttBOaECC+hakeTYgQVkL0qncgNTsx/RYbQNSs/50hUtQuFLUgBTNq21AgrwFqbYBKZpX24AEeZNRbQNSM296P44FtRzpnGZe1HiU7FdhA1LUunS8n7QiDYPjHLQbpQ1IkLcg1TYgNWO24X1YpwGpYKy0yagTlPx0k3lhc1LUWhT8sKVrdXtRk1GwVth4lLQiRQ1IQbtR1IqUtSANotai4BuP5iUtSF7SnMQ67UZJC1JtA1Kz7RxbkGobkCBrQSqZA9CN2o3SFqSSORC2ImUtSEEDkoetSLPHRsvBc46o3Wh19rywyShoH4rnJWsF250yHGwQPu94zN0vWGerh4CzJu7vH49NMbM3Au8Bftjd12btyvY/ixURERERESC/RkHBdQpuA841s3PMbAW4FDg0OcHMXg38F+CScUvoTHqlQERERESkDQ52kpkCdx+Y2ZXAzTRv9LjW3e8ys/cCh939EPCrwLOA/2lmAF9z90s2WlcnBSIiIiIiLbHgooOzuPtNwE3J2NUTH7/xZNfUSYGIiIiISBtOVJLuADop2IW2O/C8E8LO0ZbDIMQ2z7Bzs23yNSvDzuF+RGsVhJ3jr5mH/qK/sTTs3Kw1fb9fGXaGPPBcG3YGGCWP+2hOaWg5DTzXhp2jbWvDzpAHnmvDzs3Y0ob3oSzsDHnguTbsDHnguTbsDGCDJLTc20wAOrnfy/ehJOwcrVUbdm7mpaHlYLv+7LBzMzb9eyEMOwdjWdgZ8kByZdgZgsBzZdg5nLcTws6QBZ5rw87hvGBOJwgyd6IgczqvNuwMWeC5OOwchZuzAHRZ4clOZDg2qnvOtNV0UiAiIiIi0gYHC1rK2qCTAhERERGRVjjolQIRERERkQWmTIGIiIiIyIJzsChf0wKdFMhctHF150xl7qg+7JxvXRt2hvle3bk67BytVR12brae3i6aMTvsDHnguTbsHM2rDTtH86IrQEdB4+jqzulaa0EAuiTsDHnguTTsXBKArg07R2NR2Lk3CLYLxtLA8yC4AvQwCDv3giAzaeA5CBqXhJ0hDzyHweZwu3y3Osm+hleADq/kHM3zZE5d2BnywHNp2NnCeYNkTmXYGfLAc23YOVi/OuwMWeB5J4Sdm6HkauVR2DnYjm6wVhJItmDOUnQl5yDInAWeozmnDL19SERERERksTnhyWMbdFIgIiIiItIG9/DVqTbopEBEREREpC16+5CIiIiIyAJzhyhn0gKdFIiIiIiItMLjwHsLdFIgu9ZOaEDqBK0MYTtQoL4FKWhYSZotdkIDEpS1IJU0IEHeglTbgNRsm86oa0CCvAWptgGpmdctmDO7tajZ16QVKdguaiSK1kpbkErblI4HY08NV6fXrmxAasam96MXbFfSgARwbJCsVdiA1B8Exz4Zq25AAkiaiyxqRYqajKKWorQVqbIBKVq/tgEJoJts213L55Q0IDVrjZI5dQ1Izbzp3x3VDUiQPymsbECCoN2osgGpGZo+hjuhAQmCFqSoAelU4ev8fbRgU0fRzC42s3vN7IiZXbVVOyUiIiIisuudCBqntxlmPQc3s1Uz+x/jz3/ezM6etWb1SYGZdYFrgDcB5wGXmdl5teuJiIiIiCwWx4fD7LaRwufgbweedPfvBz4IvH/WnmzmlYILgSPufr+794AbgAObWE9EREREZHE4TdA4vW2s5Dn4AeD68ccfA95gZhu+eXgzJwVnAg9O3D86HhMRERERkRncnVF/kN1mKHkO/vQcdx8A3wJeuNGicw8am9kVwBXju2vdM+67c95fU0L7gMfa3okFpWPfHh379ujYt0fHvj069psRdUFE3Rzxc+bo2L9sk3s0d9/hyZv/1+jGfcGn9pjZ4Yn7B9394Dz3ZTMnBQ8BZ03c3z8emzL+Bg4CmNlhd79gE19TKunYt0fHvj069u3RsW+Pjn17dOzbc6oee3e/uGKzkufgJ+YcNbMl4LnA4xstupm3D90GnGtm55jZCnApcGgT64mIiIiIyMZKnoMfAi4ff/y3gT9w37gHvPqVAncfmNmVwM00xejXuvtdteuJiIiIiMjG1nsObmbvBQ67+yHgw8BvmdkR4AmaE4cNbSpT4O43ATedxCZzfS+UbEjHvj069u3RsW+Pjn17dOzbo2PfnoU69tFzcHe/euLj48DfOZk1bcYrCSIiIiIissudwteFFhERERGRrbAtJwWzLsUsW8fMzjKzW8zsbjO7y8zeMR5/gZl9xszuG//5/Lb3dbcys66ZfdHMfm98/5zxJcaPjC85vtL2Pu5GZvY8M/uYmf2Zmd1jZj+ox/32MLNfGP++udPMPmpme/S4nx8zu9bMHjWzOyfGwse6Nf7T+O/hDjN7TXt7fmpb57j/6vh3zh1m9jtm9ryJz71rfNzvNbOfaGWnd4no2E987p1m5ma2b3xfj/lKcz8pKLwUs2ydAfBOdz8PeB3wT8bH+yrgs+5+LvDZ8X2Zj3cA90zcfz/wwfGlxp+kufS4bL3/CHzK3f8i8EqavwM97ufMzM4E/hlwgbufTxN6uxQ97ufpOiCtMVzvsf4m4Nzx7QrgQ9u0j7vRdeTH/TPA+e7+CuDPgXcBjP/dvRT4y+Ntfn38fEjqXEd+7DGzs4AfB742MazHfKXteKWg5FLMskXc/WF3/8L44+/QPDE6k+nLXV8PvKWVHdzlzGw/8DeA3xzfN+BHaS4xDjr2c2FmzwX+Ok3bAu7ec/dvosf9dlkCTht3Ye8FHkaP+7lx91tp2kQmrfdYPwD8N298DniemZ2xLTu6y0TH3d0/Pb5aLMDnaPrioTnuN7j7mrs/AByheT4kFdZ5zAN8EPgXTF/2TI/5SttxUlByKWaZAzM7G3g18Hngxe7+8PhTXwde3NZ+7XL/geYX1IlrML4Q+ObEPxp6/M/HOcA3gP86fuvWb5rZ6ehxP3fu/hDw72n+p+5h4FvA7ehxv93We6zr3+Dt8w+AT44/1nGfMzM7ADzk7l9KPqVjX0lB413KzJ4F/Dbw8+7+7cnPjS9eodqpLWZmbwYedffb296XBbQEvAb4kLu/GvguyVuF9Lifj/F71w/QnJh9L3A6wcv8sn30WN9+ZvYemrfvfqTtfVkEZrYXeDdw9ay5Um47TgpKLsUsW8jMlmlOCD7i7h8fDz9y4uWz8Z+PtrV/u9jrgUvM7Cs0b5P7UZr3uT9v/LYK0ON/Xo4CR9398+P7H6M5SdDjfv7eCDzg7t9w9z7wcZqfBT3ut9d6j3X9GzxnZvY24M3AT01cMVbHfb6+j+Y/Ir40/jd3P/AFM3sJOvbVtuOkoORSzLJFxu9h/zBwj7t/YOJTk5e7vhz4xHbv227n7u9y9/3ufjbN4/wP3P2ngFtoLjEOOvZz4e5fBx40sx8YD70BuBs97rfD14DXmdne8e+fE8dej/vttd5j/RDwM+NGltcB35p4m5FskpldTPOW0Uvc/amJTx0CLjWzVTM7hyb0+qdt7ONu5O5fdvcXufvZ439zjwKvGf9boMd8pW25eJmZ/STNe61PXIr53879iy4oM/sh4H8DX+aZ97W/myZXcCPwUuCrwFvdPQrtyBYws4uAX3L3N5vZX6B55eAFwBeBn3b3tRZ3b1cys1fRBLxXgPuBn6X5jw897ufMzP418Hdp3j7xReAf0ryHV4/7OTCzjwIXAfuAR4BfBn6X4LE+PlH7zzRv6XoK+Fl3P9zCbp/y1jnu7wJWgcfH0z7n7v94PP89NDmDAc1beT+ZrillomPv7h+e+PxXaBrQHtNjvp6uaCwiIiIisuAUNBYRERERWXA6KRARERERWXA6KRARERERWXA6KRARERERWXA6KRARERERWXA6KRARERERWXA6KRARERERWXA6KRARERERWXD/H4oz3EY499SsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = W_loss_mat(torch.rand((3, 20, 141)).size())\n",
    "\n",
    "print(res.size())\n",
    "\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.pcolormesh(res[2])\n",
    "#plt.xlabel('Embedding Dimensions')\n",
    "#plt.xlim((0, dimensions))\n",
    "#plt.ylim((20, 0))\n",
    "#plt.ylabel('Token Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in tqdm(enumerate(train_loader)):\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    utt, utt_lens, text_ohed, text_ohed_lens, text = batch\n",
    "    true_mel_lens = get_mel_lens(utts_lens)\n",
    "    max_true_mel_len = max(true_mel_lens)\n",
    "    true_mel_lens = torch.Tensor(true_mel_lens).type(torch.long)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 139])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ohed = text_ohed.repeat(2, 1, 1)\n",
    "text_ohed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[139, 112, 116]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ohed_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-410-022168d0acbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ohed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_ohed_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mbatch_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "a = torch.nn.utils.rnn.pack_padded_sequence(text_ohed, lengths=text_ohed_lens, batch_first=True, enforce_sorted=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[33, 12, 12,  8,  5,  9,  2, 10, 10,  8, 15, 10,  2, 10,  8,  5,  2,  3,\n",
       "          10, 19,  8, 17,  5,  8,  5,  9, 10,  8, 29, 10,  4,  5,  2, 17, 12,  8,\n",
       "          29,  2,  3, 21,  3,  4, 17, 12,  8, 29, 11, 28,  2,  5,  7,  8, 17,  4,\n",
       "          19,  8, 14, 10,  4,  5, 10,  4, 16, 10, 19,  8,  5, 11,  8, 20, 11, 28,\n",
       "           2,  5, 10, 10,  4,  8, 13, 10, 17,  2, 14, 54,  8,  5,  2, 17,  4, 14,\n",
       "          18, 11,  2,  5, 17,  5,  3, 11,  4,  7,  8, 18, 17, 14, 14,  3,  4,  6,\n",
       "           8, 14, 11, 21, 10,  8,  5,  3, 21, 10,  8,  3,  4,  8, 31, 10, 15,  6,\n",
       "          17,  5, 10,  8, 10,  4,  8,  2, 11, 28,  5, 10, 26],\n",
       "         [40,  9, 10, 14, 10,  8, 16, 11, 21, 18, 17,  2,  3, 14, 11,  4, 14,  8,\n",
       "          15,  3, 12, 12,  7,  8,  9, 11, 15, 10, 25, 10,  2,  7,  8, 24, 10,  8,\n",
       "           5,  2, 17,  4, 14, 12, 17,  5, 10, 19,  8,  3,  4,  5, 11,  8,  5, 10,\n",
       "           2, 21, 14,  8, 17, 18, 18, 12,  3, 16, 17, 24, 12, 10,  8,  5, 11,  8,\n",
       "          17,  4, 13,  8, 14, 18, 10, 16,  3, 10, 14,  8, 11, 20,  8,  9,  3,  6,\n",
       "           9, 10,  2,  8, 18, 12, 17,  4,  5, 14,  8, 11,  2,  8, 17,  4,  3, 21,\n",
       "          17, 12, 14, 26, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75,\n",
       "          75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75],\n",
       "         [ 5,  9, 11, 14, 10,  8, 17,  6, 17,  3,  4, 14,  5,  8,  5,  9, 10,  8,\n",
       "          18, 10,  2, 14, 11,  4,  8, 24, 10, 16, 17, 21, 10,  8, 21, 11,  2, 10,\n",
       "           8, 17,  4, 19,  8, 21, 11,  2, 10,  8, 12,  3, 21,  3,  5, 10, 19,  8,\n",
       "           5, 11,  8,  5,  9, 10,  8, 21, 11, 14,  5,  8,  9, 10,  3,  4, 11, 28,\n",
       "          14,  7,  8, 11,  2,  8,  5,  9, 11, 14, 10,  8, 15,  9,  3, 16,  9,  8,\n",
       "          21, 10,  4, 17, 16, 10, 19,  8, 11,  2,  8, 19, 10, 14,  5,  2, 11, 13,\n",
       "          10, 19,  8, 12,  3, 20, 10, 26, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75,\n",
       "          75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75]]),\n",
       " tensor([139, 112, 116]))"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.rnn.pad_packed_sequence(a, batch_first=True, padding_value=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(text_ohed_lens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-ht2]",
   "language": "python",
   "name": "conda-env-dl-ht2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
